---
title: "Projet données manquantes : Valeurs des logements en banlieue de Boston"
author: "Olga Silva / Marlène Chevalier"
date: "30/11/2019"  
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#packages utilisés
library("missMDA")
library("VIM")

library("mice")
library(miceadds)
library("lavaan")
library(kableExtra)

# chargement des données
dHB=read.table("HousingData.csv",sep=",",header=TRUE)
attach(dHB)
```

## Sujet : Valeur du logement en banlieue de Boston 

Il s'agit de traiter les données manquantes du fichier Boston Housing. Ce fichier décrit la situation des logements dans les villes de la banlieue de Boston. Il est constitué de 506 enregistrements et de 14 variables quantitatives (soit 7084 données) :

 - **CRIM** : Taux de criminalité par habitant.   
 - **ZN** : Proportion de terrains résidentiels pour des lots de plus de 25000 pieds carré (environ 2300m²).   
 - **INDUS** : Proportion d'espace, en acres, consacré aux affaires non commerciales (1 acre environ 4000m²).   
 - **CHAS** : Proximité avec la rivière Charles (=1 si en bord de rivière / =0 si éloigné de la rivière) 
  - **NOX** : Concentration en oxyde d'azote (1 pour 10 millions)  
  - **RM** : Nombre moyen de chambres par logement  
  - **AGE** : Proportion des propriétés construites avant 1940
  - **DIS** : Moyenne des distances aux 5 centres d'emploi de Boston  
  - **RAD** : Indice d'accessibilité aux autoroutes (de 1 à 8 et 24)  
  - **TAX** : Taux d'imposition foncier (1 pour 10 000$)  
  - **PTRATIO** : Ratio d'élèves-enseignants  
  - **B** : Proportion de population afro-américaine
  - **LSTAT** : Proportion de population précaire 
  - **MDEV** : Valeur médiane des habitations privées (en K$) 
  
Nous utiliserons ces données pour tenter d'expliquer la valeur médiane des habitations privées (MDEV) en fonction des autres variables du fichier.
  
# Exploration des données incomplètes
## Graphiques sur les données incomplètes

```{r graDM, echo=FALSE}
op=par(mfrow = c(2, 2), pch = 16, col="green")
{plot(CRIM,MEDV)
plot(ZN,MEDV)}
plot(INDUS,MEDV)
plot(NOX,MEDV)
par(op)
title("valeur des habitations (K$)")
par(mfrow = c(2, 2), pch = 16, col="green")
plot(RM,MEDV)
plot(AGE,MEDV)
plot(DIS,MEDV)
plot(RAD,MEDV)
par(mfrow = c(2, 2), pch = 16, col="green")
plot(TAX,MEDV)
plot(PTRATIO,MEDV)
plot(B,MEDV)
plot(LSTAT,MEDV)
par(mfrow = c(1, 2), pch = 16, col="green")
boxplot(CHAS,MEDV)
```

## Correlation

```{r cor, echo=FALSE}
cordBH=cor(dHB,use="complete.obs")
kable(head(cordBH),format="markdown",caption="correlation des var Housing Boston",digits=2)
```


## Modélisation sur les données incomplètes

```{r modDM, echo=FALSE}
reg1=lm(MEDV~.,data=dHB,na.action=na.omit)
summary(reg1)
##reg2=lm(MEDV~.-INDUS-AGE,data=dHB)
##summary(reg2)
```

# Inventaire des données manquantes

Nous allons utiliser la fonction 'md.pattern' du package mice pour analyser les données manquantes. Le résultat est une matrice, dans laquelle chaque ligne correspond à des structures de données manquantes et chaque colonne à une variable du fichier. Les lignes et les colonnes sont triées selon le niveau de complétude des données.   
A chaque ligne de la matrice (qui définit une structure de données manquantes du jeux de données) :  
  - la première colonne indique le nombre d'observations correspondant à la structure de données manquantes décrite ;   
  -  la derniere colonne donne le nombre de variables incomplètes.

```{r inventDM1,echo=TRUE}
respattern=md.pattern(dHB,rotate.names = TRUE)
```

Au total, 120 observations sont manquantes : 20 pour chacune des variables CRIM, ZN, INDUS, CHAS, AGE, LSTAT. 

  - structure 1 : 394 observations pour lesquelles aucune donnée n'est manquante,
  - structure 2 : 18 observations pour lesquelles seule la donnée de la variable LSTAT est absente,
  - structure 3 : 18 observations pour lesquelles seule la donnée de la variable AGE est absente,   
  ...
  - structure 14 (dernière) : 1 observation pour laquelle les données des variables CRIM et CHAS sont absentes.

```{r inventDM2,echo=FALSE}
aggr(dHB)

```

```{r manq,echo=TRUE}
flux(dHB)[,1:3]
```

The influx of a variable quantifies how well its missing data connect to the observed data on other variables. The outflux of a variable quantifies how well its observed data connect to the missing data on other variables. In general, higher influx and outflux values are preferred.


# Traitement des données manquantes

## Suppression par paire (pairwise deletion)

La suppression par paire est une autre alternative à la suppression simple des lignes avec des données manquantes. Avec cette méthode, il faut calculer la moyenne, variance et covariance de toutes les données observées.
Sous l’hypothèse de MCAR, les estimations de la moyenne, corrélations et covariances semblent consistants. Par contre, cette méthode pourrait poser des problèmes sur les variables très corrélées entre elles et si les données ne suivent pas une distribution normale. 

```{r modimpreg}
mu <- colMeans(dHB, na.rm = TRUE)
cv <- cov(dHB, use = "pairwise")

impreg <- lavaan("MEDV~1 + CRIM + ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT
              MEDV ~~ MEDV",
             sample.mean = mu, sample.cov = cv,
             sample.nobs = sum(complete.cases(dHB)))

```

D'autres solutions sont possibles aussi, comme remplacer les données manquantes par la moyenne de la variable, ou en faisant une régression avec les données observées. Même s’il s’agit des solutions rapides ces deux méthodes sont à éviter car cela modifie la corrélation entre les variables, la distribution, la variable est sous-estimée, entre autres problèmes.  

## Imputation par regression stochastique

Cette méthode consiste à imputer les données manquantes en utilisant la regression à laquelle on a ajouté du bruit. Cela permet de corriger le biais de corrélation qui existe par les méthodes plus rapides (decrites plus haut)

```{r modimpregsto}

impregsto <- mice(dHB, method = "norm.nob", m = 1, maxit = 1,
            seed = 1, print = FALSE)
summary(impregsto)
```

## Completion des données manquantes

Rubin (1976) a classé les problèmes des données manquantes en trois catégories : 
* Données manquantes de façon complètement aléatoire (missing completely at random, MCAR). La cause d'absence des données ne pas être liée au data. Même si pratique, cette hypothèse est peu réaliste. 
* Données manquantes de façon aléatoire (missing at random, MAR). La probabilité d’absence de la valeur d’une variable dépend des valeurs prises par d’autres variables qui ont été observées
* Données manquantes de façon non aléatoire (missing not at random, MNAR). La probabilité d’absence de la valeur d’une variable dépend de variables qui n’ont pas été observées. 

La plupart des corrections simples fonctionnent sous l'hypothèse peu réaliste de MCAR. 
MAR est plus générale et plus réaliste que MCAR. La plupart des méthodes modernes de traitement des données manquantes partent de la supposition MAR.

Schafer and Graham (2002, 156) ont proposé que si le problème des données manquantes peut être traité en enlevant une petite partie de l'échantillon, alors la méthode peut être efficace.  

L'imputation multiple est maintenant acceptée comme une méthode pour traiter des données non complètes. L'imputation correspond à l'action de convertir un échantillon non complet dans un échantillon complet. Le but de l'imputation multiple est d'imputer plusieurs fois les données manquantes, analyser les données complétés et ensuite intégrer les résultats des analyses.  
 

## Multiple imputation

**attention la méthode est à modifier, j'ai laissé la méthode par défaut "pmm"!!!**

Le choix de m : L'imputation du dataset demande de faire des "essais-erreur", pour adapter et améliorer le modèle. Pour démarrer il est conseillé de mettre m = 5 et l'augmenter lors de la dernière étape si on est déjà satisfait avec le modèle. 

```{r mult_imputation, echo=TRUE}
#classic recommended workflow

imp <- mice(dHB, print = FALSE, m = 10, seed = 24415)
fit <- with(imp, lm(MEDV~1+CRIM + ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT))
#with : fill in the missing data and to analyze the data
est <- pool(fit)
est

```

La colonne "estimate" correspond à la valeur Q. Les colonnes "ubar", "b" et "t" sont les estimateurs des variances. "dfcom" correspond aux degrés de liberté du dataset comple, df est les degrés de liberté après la correction de Barnard-Rubin. Les trois dernières colonnes sont l'augmentation relative de la variance r, la proportion de la variance dû au non réponse λ et γ la fraction d'information manquante par paramètre. 

λ est différent de 1, c'est qui pouvait arriver si toute la variation était causé par les données manquantes. 

Les montants λ, r et γ, sont des indicateurs de la gravité du problème d'absence des données. Fractions d'information manquante de 0 - 0.2 sont interprétés comme "modeste", jusqu'à 0.3 comme "moyennement grand" et 0.5 comme "élevé" (Li, Raghunathan, and Rubin 1991). Des valeurs élevées indiquent un problème difficile dans lequel les inférences statistiques dépendent fortement de la manière dont les données manquantes ont été traitées.

Aucune de nos valeurs est supérieur à 0.09, le problème est alors modeste.

Des test de Wald et des intervalles de confiance pour chaque element de Q sont des résultats standard de chaque procédure statistique. 

```{r conf_interv, echo=TRUE}

summary(est, conf.int = TRUE)

```

Selon ces résultats, il semblerait que 'INDUS' et 'AGE' ne soient pas des prédicteurs statistiquement pertinents pour prédire MEDV, avec un erreur de type 1 de 5%.
Ce résultat est cohérent par rapport aux résultats de la LM avec des données manquantes.

##Stepwise selection

L'imputation multiple consiste en trois phases :
1 - Imputation des données manquantes m fois  
2 - Analyse de m datasets imputés  
3 - Mise en commun des paramètres à travers m analyses  

```{r model, echo=TRUE, results="hide"}

imp <- mice(dHB, seed = 123, m = 10, print = FALSE)
scope <- list(upper=~CRIM + ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT,
              lower = ~1)
expr <- expression(f1 <- lm(MEDV ~ 1),
                   f2 <- step(f1, scope = scope))
fit <- with(imp, expr)

formulas <- lapply(fit$analyses, formula)
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
table(votes)

```
La fonction table compte le nombre de fois que chaque variable est présente dans les itérations. Le meilleur modèle semble être quand toutes les variables sont presentes lors des 10 simulations. 

## Critères d'évaluation

1 - Raw bias (RB) and percent bias (PB). Le biais brut de Q est defini comme la différence entre l'espérance de l'estimateur et le réel. Il doit être proche de zéro.     
2 - Coverage rate (CR) est la proportion d'intervalles de confiance qui contient la valeur réelle.Elle doit être proche de 0.95.  
3 - Average width (AW), c'est un indicateur de l'efficacité statistique. La largeur doit être le plus petite possible, sans que le CR soit plus bas que la valeur nominale. 
Des méthodes sans biais et avec un bon CR sont connues comme "randomization-valid" (Rubin 1987b).  


## Conclusion


## Annexes


