---
title: "Projet : Méthodes de traitement de données manquantes"
author: "Olga Silva / Marlène Chevalier"
date: "30/11/2019"  
output:
  html_document: default
---

<style type="text/css">
body{ /* Normal  */
font-size: 12px;
}
td {  /* Table  */
font-size: 12px;
}
h1.title {
font-size: 26px;
}

h1 { /* Header 1 */
font-size: 20px;
}
h2 { /* Header 2 */
font-size: 16px;
font-weight:bold;
}
h3 { /* Header 3 */
font-size: 14px;
font-weight:bold;
}
</style>



```{r setup, warning=FALSE, include=FALSE}

#packages utilisés
library(knitr)
library(missMDA)
library(VIM)
library(MASS)
library(stats)
library(micemd)

library(plotly)
library(fitdistrplus)
library(FactoMineR)
library(mice)
library(miceadds)
library(lavaan)
library(kableExtra)
library(dplyr)
library(missForest)

# options globales
opts_chunk$set(echo=FALSE,warning=FALSE)

# chargement des données
dHB=read.table("HousingData.csv",sep=",",header=TRUE)
attach(dHB)
```

# Sujet : Valeur du logement en banlieue de Boston 

Il s'agit de traiter les données manquantes du fichier Boston Housing. Ce fichier décrit la situation des logements dans les villes de la banlieue de Boston. Il est constitué de 506 enregistrements et de 14 variables quantitatives (soit 7084 données) :

 - **CRIM** : Taux de criminalité par habitant.   
 - **ZN** : Proportion de terrains résidentiels pour des lots de plus de 25000 pieds carré (environ 2300m²).   
 - **INDUS** : Proportion d'espace, en acres, consacré aux affaires non commerciales (1 acre environ 4000m²).   
 - **CHAS** : Proximité avec la rivière Charles (=1 si en bord de rivière / =0 si éloigné de la rivière) 
  - **NOX** : Concentration en oxyde d'azote (1 pour 10 millions)  
  - **RM** : Nombre moyen de chambres par logement  
  - **AGE** : Proportion des propriétés construites avant 1940
  - **DIS** : Moyenne des distances aux 5 centres d'emploi de Boston  
  - **RAD** : Indice d'accessibilité aux autoroutes (de 1 à 8 et 24)  
  - **TAX** : Taux d'imposition foncier (1 pour 10 000$)  
  - **PTRATIO** : Ratio d'élèves-enseignants  
  - **B** : Proportion de population afro-américaine
  - **LSTAT** : Proportion de population précaire 
  - **MDEV** : Valeur médiane des habitations privées (en K$) 
  
Nous utiliserons ces données pour tenter d'expliquer la valeur médiane des habitations privées (MDEV) en fonction des autres variables du fichier.
  
# 1.Exploration des données incomplètes

## Graphiques sur les données incomplètes

```{r stat}
moyMEDV=round(mean(MEDV),1)
sdMEDV=round(sd(MEDV),1)
minMEDV=min(MEDV)
maxMEDV=max(MEDV)

``` 
La valeur médiane du logement à Boston est comprise entre `r minMEDV` K$ et `r maxMEDV` K$ , en moyenne de `r moyMEDV` K$.

```{r expl}

par(mfrow = c(1, 1), pch = 16, col="blue")
hist(MEDV,main=NULL,xlab="valeur médiane du logement(K$)", ylab="fréquence", col="blue")
abline(v=mean(MEDV), col="red")

```

Graphiquement (*cf.annexe 1.1*), on observe que :   
   
   - Le **taux de criminalité** faible (inféreur à 10%) est le plus fréquent. La valeur du logement a tendance a diminué lorsque le taux de criminalité augmente. Mais la corrélation entre les 2 reste faible (0.4).  
   - La **proportion de terrains résidentiels** est majoritairement faible (inféreur à 10%), mais lorsqu'elle augmente, la valeur du logement a tendance a augmenté.  
   - La **proportion de surface d'activité industrielle** la plus fréquente est entre 18 et 20 acres (entre 72000m² et 80000m²). A ce niveau, la valeur moyenne est bien souvent inférieure à la valeur médiane moyenne des logements (22.5K$).  
   - La **concentration d'oxyde d'*azote** est le plus souvent entre 0.4 et 0.6. Plus la concentration augmente, plus la valeur des logements diminue.  
   - Le **nombre moyen de chambre** est le plus souvent entre 5 et 7. Plus le nombre de chambre augmente, plus les logements ont de la valeur.   
   - La **proportion de propriétés construites avant 1940** est très importante (majoritairement autour de 90% ). Plus cette proportion augmente, plus les logements ont de la valeur.  
   - La **moyenne des distances aux centres d'emploi** est fréquemment faible (<4). Cette variable influence peu la valeur des logements (corrélation=0.28).  
   - L'**imposition foncière** prend la plus importante partie de ses valeurs entre 200 et 500. Puis une autre série importante de ses valeurs est autour de 666 ; à ce niveau d'impôt, la valeur du logement est plus faible (<moyenne 22.5).  
   - Le **ratio élèves-enseignants** est réparti quasi-équitablement autour de la valeur moyenne du logement.Une hausse de ce ratio a tendance à faire baisser le prix du logenment (corrélation =-0.54)
   - La **proportion de population afro-américaine** est importante; mais son influence n'est pas significative sur la valeur du logement (cor =0.35)
   - La **proportion de population précaire** influence négativement la valeur des logements (cor =-0.74).
   - L'**indice d'accessibilité aux autoroutes** à 24 donne les valeurs des logements les plus basses (15K$ en moyenne) . Les indices 3 et 8 donnent les valeurs de logement les plus élevées.  
   - La **proximité avec la rivière Charles** augmente légèrement la valeur du logement.
    
## Corrélation des variables

Les corrélations les plus significatives de la valeur du logement sont avec :  

   - RM (0.72) : plus le nombre de chambre est important, plus la valeur du biens est forte.  
   - INDUS et RAD (-0.51) : plus l'espace d'affaires non commerciales ou plus l'indice d'accessibilité aux autoroutes seront importants, moins la valeur du bien sera élevée.    
   - PTRATIO (-0.54) : plus le ratio elèves-enseignants est fort, moins la valeur des biens est élevée.  
   - LSTAT (-0.74) : une forte proportion de population précaire réduira très fortement la valeur des biens.  
(*cf. annexe 1.2*)

## Modélisation sur les données incomplètes

Nous commençons par examiner les résultats d'une regression linéaire de MEDV sur les autres variables. Le modèle est correctement ajusté (R²=0.75) mais il semble que les variables explicatives INDUS et AGE ne soient pas pertinentes pour ce modèle. (*cf. annexe 1.3*)

Nous allons appliquer une méthode "step AIC" pour choisir le meilleur modèle avec le risque de perte d'information et d'introduction de biais au dataset.   
Ce modèle écarte les variables INDUS et AGE avec un R2 ajusté très proche de l'original. (*cf. annexe 1.4*)

## En conclusion sur l'exploration de données
La valeur médiane du logement à Boston et ses environs est comprise entre 5 et 50 Kdollars. Sa distribution est croissante jusqu'a sa valeur moyenne (22.5Kdollars) puis décroit fortement à partir de 25 K$.  
**La valeur du logement est influencée ** (p_value <5%): 

  **- positivement** principalement (coefficient estimé = 4.2) **par le nombre de chambres** et plus faiblement (coefficients estimés de 0.28 et 0.05) **par l'accessibilité aux autoroutes et la proportion de terrain résidentiel**   
  
  **- négativement** principalement (coefficient estimé = -18.5) **par la concentration en oxyde d'azote** et plus faiblement (coefficients estimés entre de -1.42 et -0.01) **par la distance aux centres d'emplois, le ratio élèves-enseignants, la proportion de population précaire et le taux foncier**   
Le R² du modèle de regression sur jeu de données incomplet est de **0.76 (R² de référence)**.    
(**cf. annexe 1.4** : résultat de la regression linéaire après selection de variable)

Les variables INDUS et AGE ne sont pas significatives dans l'explication de la valeur du logement (p-value>5% **cf. annexe 1.3**).

Nous allons comparer maintenant ce modèle obtenu avec des données manquantes et les nouveaux modèles que nous allons obtenir avec des datasets complets à partir de differentes méthodes. 


# 2.Inventaire des données manquantes 
Il s'agit ici d'identifier les données manquantes par variable, représenter leur structure dans le jeu de données.

## Structure des données manquantes

La fonction **md.pattern** du package a pour résultat une matrice, dans laquelle chaque ligne correspond à des structures de données manquantes et chaque colonne à une variable du fichier. Les lignes et les colonnes sont triées selon le niveau de complétude des données.   
A chaque ligne de la matrice (qui définit une structure de données manquantes du jeu de données) :  

  - la première colonne indique le nombre d'observations correspondant à la structure de données manquantes décrite ;    
  - la derniere colonne donne le nombre de variables incomplètes.

```{r inventDM1}
respattern=md.pattern(dHB,rotate.names = TRUE)
```

Au total, 120 observations sont manquantes : 20 pour chacune des variables CRIM, ZN, INDUS, CHAS, AGE, LSTAT. 

  - structure 1 : 394 observations pour lesquelles aucune donnée n'est manquante,
  - structure 2 : 18 observations pour lesquelles seule la donnée de la variable LSTAT est absente,
  - structure 3 : 18 observations pour lesquelles seule la donnée de la variable AGE est absente,   
  ...
  - structure 14 (dernière) : 1 observation pour laquelle les données des variables CRIM et CHAS sont absentes.
 
Voici une deuxième representation des données manquantes obtenue avec **matrixplot**. Elle permet d'identifier des dépendences des valeurs extrêmes et des données manquantes. Les données observées sont en gris et les manquantes en rouge. 

```{r inventDM3}
matrixplot(dHB,sortby="MEDV")
```

Nous pouvons observer que les valeurs manquantes se répartissent bien dans l'ensemble du jeu de données.
 
## Proportion de données manquantes

La fonction **aggr** permet d'appréhender les données par leur proportion dans le jeu complet.

```{r inventDM2}
aggr(dHB,numbers=TRUE,bars=FALSE)
```

En sortie, 2 graphiques :  

  - Le graphique de gauche donne la proportion de données manquantes de chaque variable : ici on retrouve des proportions égales pour les variables CRIM, ZN, INDUS, CHAS, LSTAT (autour de 4%), les autres variables sont complètes.    
  - Le graphique de droite donne la proportion de chaque structure de données.   
Ici 78% du jeux de données est complet pour toutes les variables, 3.8% des individus ont uniquement la variable CRIM qui n'est pas renseignée, ...

## Catégories de données manquantes

Rubin (1976) a classé les problèmes des données manquantes en trois catégories :  

  - **Données manquantes de façon complètement aléatoire : MCAR** (missing completely at random). L'absence de  données est dûe au hasard, à la malchance. Cette hypothèse est peu réaliste.  
  - **Données manquantes de façon aléatoire : MAR** (missing at random). La probabilité d’absence de la valeur d’une variable dépend des valeurs prises par d’autres variables qui ont été observées. MAR est plus générale et plus réaliste que MCAR.
  - **Données manquantes de façon non aléatoire : MNAR ** (missing not at random). La cause d’absence de la valeur d’une variable est de raision inconnue. MNAR est le cas le plus complexe.

La plupart des méthodes modernes de traitement des données manquantes partent de la supposition MAR. Dans le cas du jeu de données Boston Housing, on peut aussi partir de cette hypothèse (il nous manque que 22% des données , donc ici nous sommes loin du point problèmatique du 50%)


## En conclusion sur l'inventaire des données manquantes 

  - 6 variables sont incomplètes, avec chacune 20 données manquantes (soit 120 au total) :

     - **CRIM** : Taux de criminalité par habitant     
     - **ZN** : Proportion de terrains résidentiels  
     - **INDUS** : Proportion d'espace consacré aux affaires non commerciales  
     - **CHAS** : Proximité avec la rivière Charles  
     - **AGE** : Proportion des propriétés construites avant 1940  
     - **LSTAT** : Proportion de population précaire  
   
Sur le jeu de données, 22% des individus sont incomplets  

  - Nous supposons qu'on est en situation **MAR (Données manquantes de façon aléatoire)**

# 3.Traitement des données manquantes

## Méthode de suppression

Une méthode de traitement des données manquantes peut être la suppression des lignes avec des valeurs absentes, pour avoir un dataset complet. La suppression par paire **lavaan** est une alternative à la suppression simple des lignes avec des données manquantes. Avec cette méthode, il faut calculer la moyenne, variance et covariance de toutes les données observées.
Cette méthode pourrait poser des problèmes sur les variables très corrélées entre elles et si les données ne suivent pas une distribution normale. 

Nous observons que cette méthode, comme prévu, va supprimer les lignes avec des données manquantes, donc le dataframe d'analyse se réduit à 394 lignes. Selon les résultats du p-value, INDUS et AGE (p-value > 5%) ne sont pas representatives ; elles peuvent donc sortir du modèle (**cf.annexe 3.1**).  

Par contre c'est une méthode à eviter, car nous allons perdre des informations et nous pouvons ajouter des biais au modèle. 
Nous préférons continuer notre recherche sur les méthodes d'imputation.

## Imputation simple

Les méthodes d'imputation simples consistent à remplacer chaque valeur maquante par une valeur unique prédite ou simulée. 
Plusieurs solutions sont possibles (remplacer les données manquantes par la moyenne de la variable, faire une régression avec les données observées...). Ces solutions rapides sont à éviter car cela modifie la corrélation entre les variables, la distribution des variables, la variable peut-être sous-estimée, entre autres problèmes. 

Nous allons ici en tester 2 méthodes d'imputation simple : l'imputation par regression stochastique, les forêts aléatoires . Nous allons appliquer ces méthodes à des modèles sans INDUS et AGE, car elles ne semblent pas pertinentes, selon les résultats des analyses précèdentes. 


### Imputation par régression stochastique

Cette méthode consiste à imputer les données manquantes en utilisant la regression à laquelle on a ajouté du bruit. Cela permet de corriger le biais de corrélation qui existe par les méthodes plus rapides (décrites plus haut).  
fonction **mice ( avec method = "norm.nob")**
(cf. **annnexe 3.3**)

```{r modimpregsto}
impregsto = mice(dHB, method = "norm.nob", m = 6, maxit = 60,
            seed = 600, print=FALSE)
dHBcompl.regsto=complete(impregsto)
```
Avec cette méthode, on observe un R2 ajusté de 0.736. Proche mais plus bas que le R2 ajusté de la regression faite avec les données manquantes.

### Imputation par forêts aléatoires

Avec cette méthode, pour les variables continues les valeurs sont imputées en faisant des tirages aléatoires à partir des distributions gaussiennes independantes, centrées en les moyennes prédites par les forêts aléatoires.  
fonction **mice ( avec method = "rf")**

```{r modimpforest}

impforest = mice(dHB, method = "rf", m = 6, seed = 600, print=FALSE)
dHBcompl.rf=complete(impforest)
summary(dHBcompl.rf)
regrf=lm(MEDV~.-INDUS-AGE,data=dHBcompl.rf)
summary(regrf)

```

Avec cette méthode on observe un R2 ajusté de 0.73. Proche mais plus bas que le R2 ajuste de la regression faite avec les données manquantes. Les valeurs estimées et les résidus restent aussi très proches.


## Imputation multiple

L'imputation multiple consiste en trois phases :

**Etape 1** : Imputation des données manquantes m fois  
**Etape 2** : Analyse de m datasets imputés  
**Etape 3** : Mise en commun des paramètres à travers m analyses  

### Imputation par predictive mean matching

La méthode pmm est très répandue car elle est facile à utiliser, elle sélectionne un petit ensemble des "donneurs" à partir des cas complets, avec des valeurs prédits proches des valeurs prédits par les données manquantes. Un donneur est pris aléatoirement et la valeur observée remplace la donnée manquante.   
fonction **mice ( avec method = "pmm")**

```{r modpmm}
imppmm = mice(dHB, method = "pmm", m = 6, seed = 600, print=FALSE)
dHBcompl.pmm=complete(imppmm)
summary(dHBcompl.pmm)
regpmm=lm(MEDV~.-INDUS-AGE,data=dHBcompl.pmm)
summary(regpmm)

```

Pareil que pour les deux méthodes précédentes, le R2 ajusté est très proche du modèle avec des données manquantes. Les valeurs estimées et les résidus restent aussi très proches.

## Imputation par ACP

Les données manquantes sont imputées en utilisant la ACP (Analyse des composants principales). Il s'agit :  

   - d'estimer le nombre de dimensions utilisées dans la formule de reconstruction avec la fonction **estim_ncpPCA(dHB,method.cv = "Kfold")** 
  
```{r dim_pca}
nb.kfold = estim_ncpPCA(dHB,method.cv = "Kfold")
plot(nb.kfold$criterion~names(nb.kfold$criterion),xlab="nb de critères",type="b")
```
Selon le graphique, le nombre de dimensions sera 3.

   - générer les ensembles de données imputées avec la fonction MIPCA en utilisant le nombre de dimensions précédemment calculé  
  fonction **MIPCA (avec method.mi = "Bayes")**  
  
   - visualiser les ensembles de données imputées avec la fonction plot.MIPCA  
  
```{r pca}
res.bayesMIPCA = MIPCA(dHB,ncp=nb.kfold$ncp,verbose=TRUE,method.mi = "Bayes")
res.overmipca = Overimpute(res.bayesMIPCA)
#plot.MIPCA(res.bayesMIPCA) 

```
Selon les graphiques les intervalles touchent tous la ligne diagonale, sauf pour les plus grandes valeurs de CRIM. Il semblerait que cette méthode soit plus appropiée pour faire de l'imputation sur notre dataset. 

**Regression linéaire sur le jeu de données complété par MIPCA**

```{r lm_pca}
imppca= prelim(res.mi = res.bayesMIPCA, X = dHB)
fit= with(data=imppca,exp = lm(MEDV~CRIM+ZN+CHAS+NOX+RM+DIS+RAD+TAX+PTRATIO+B+LSTAT))
res.pool=pool(fit)
summary(res.pool)

```

Les valeurs des estimateurs sont très proches aux celles trouvées par l'imputation simple, mais les imputations multiples respectent mieux la distribution avec cette méthode. 

# 4. Diagnostics et conclusion

## Diagnostic 1 : vérifier que la distribution des données imputées est similaire à celle des données d'origine. 

```{r diagnostics1}

densityplot(impregsto, main="Régression Stochastique", layout = c(2, 3))
densityplot(impforest, main="Forêts aléatoires", layout = c(2, 3))
densityplot(imppmm, main="Predictive mean matching", layout = c(2, 3))
densityplot(imppca, main="ACP", layout = c(2, 3))
```

Pour la variable CRIM, aucune méthode ne semble parvenir à réproduire la même distribution. Par contre pour INDUS, AGE et LSTAT, les trois méthodes semblent réussir à le faire. Pour CHAS et ZN, les forêts aléatoires sont les plus proches de la vraie distribution.

## Diagnostic 2 : vérifier la convergence des algorithmes. 

La verification de la convergence se fait à partir des graphiques de variations de la moyenne et de l'écart type, pour chaque  méthode, pour chaque itération et chaque donnée imputée.Pour que la convergence soit vérifiée, il faut que les différentes courbes se mélangent librement, sans une tendance particulière. C'est bien le cas pour nos graphiques; donc, nous n'avons pas de problème de convergence. (**cf.annexe 4.1**)

## Diagnostic 3 : vérifier l'ajustement du modèle d'imputation
Pour cela, nous traçons le graphe d'overimputation. Dans ce cas, chaque donnée observée est supprimée et pour chacune d'entre elles, 100 valeurs sont predites (en utilisant la même méthode d'imputation choisie); la moyenne et des intervalles de confiance de 90% sont calculés pour ces valeurs. 

Sur ces graphiques, la 1ere bissectrice  (y=x) représente l'imputation parfaite. La qualité de l'imputation se mesure en observant la proximité des intervalles de confiance avec cette droite. On espère que 90% des intervalles traversent la 1ere bissectrice. La couleur des intervalles represente la fraction des données manquantes (entre 0 - 20% pour notre cas)

**Ajustement de la regression stochastique :**

```{r diagnostics3a}
res.over1<-overimpute(impregsto,nnodes=7,plotvars = c(1,2,3,4,7,13),plotinds=sample(x = seq(nrow(dHB)),size = 100))

```

**Ajustement des forêts aléatoires :**

```{r diagnostics3b}
res.over2<-overimpute(impforest,nnodes=7,plotinds=sample(x = seq(nrow(dHB)),size = 100))

```

**Ajustement de la predictive mean matching :**

```{r diagnostics3c}

res.over3<-overimpute(imppmm,nnodes=7,plotinds=sample(x = seq(nrow(dHB)),size = 100))
```

**Ajustement de l'ACP :**

```{r diagnostics3d}

res.over3<-overimpute(imppca,nnodes=7,plotinds=sample(x = seq(nrow(dHB)),size = 100))
```


Comme CHAS n'est pas une variable continue, elle prend uniquement les valeurs 0 et 1, elle n'apparaît pas dans les graphiques. 

On observe que pour les variables INDUS, AGE et LSTAT la plupart des intervalles de confiance contiennent bien la ligne diagonale pour chacune des méthodes. Cependant, ce n'est pas le cas pour les variables CRIM et ZN.

## Conclusion
(à faire)
Les résultats de ces méthodes d'imputation nous conduisent vers des modèles qui sont très similaires, avec les mêmes estimateurs, mêmes résidus et le même R2 ajusté. Cependant la méthode des forêts aléatoires semble mieux respecter la distribution des variables.   



**_________________________________________________________________________________**    

# Annexes  

### Annexe 1.1 : graphiques exploratoires du jeu de données
Les répresentations graphiques donnent, pour chaque variable explicative, la distribution sous forme d'histogramme et le nuage de point de la variable vs la valeur médiane du logement (en rouge la valeur moyenne = 22.5K$).

```{r angraDM}

par(mfrow = c(1, 2), pch = 16, col="green")
hist(CRIM,main=NULL,xlab="taux de criminalité",ylab="fréquence", col="green")
plot(CRIM,MEDV,xlab="taux de criminalité",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et taux de criminalité")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(ZN,main=NULL,xlab="prop.terrains résidentiels",ylab="fréquence", col="green")
plot(ZN,MEDV,xlab="prop.surface activité industrielle",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title(main="Valeur médiane du logement et proportion de terrains résidentiels")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(INDUS,main=NULL,xlab="prop.surface activité industrielle",ylab="fréquence", col="green")
plot(INDUS,MEDV,xlab="prop.surface activité industrielle",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title(main="Valeur médiane du logement et proportion de surface d'activité industrielle")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(NOX,main=NULL,xlab="concentration oxyde d'azote (1/10M)",ylab="fréquence", col="green")
plot(NOX,MEDV,xlab="concentration oxyde d'azote (1/10M)",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et concentration en oxyde d'azote")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(RM,main=NULL,xlab="nombre de chambre",ylab="fréquence", col="green")
plot(RM,MEDV,xlab="nombre de chambre",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et nombre moyen de chambre")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(AGE,main=NULL,xlab="prop. construites avant 1940",ylab="fréquence", col="green")
plot(AGE,MEDV,xlab="prop. construites avant 1940",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et proportion de propriétes construites avant 1940")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(DIS,main=NULL,xlab="distance moyenne aux centres d'emplois",ylab="fréquence", col="green")
plot(DIS,MEDV,xlab="distance moyenne aux centres d'emplois",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et distance moyenne aux 5 centres d'emplois")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(TAX,main=NULL,xlab="taux d'imposition foncier (1/10K$",ylab="fréquence", col="green")
plot(TAX,MEDV,xlab="taux d'imposition foncier (1/10K$",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et taux d'imposition foncier")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(PTRATIO,main=NULL,xlab="ratio élèves-enseignants",ylab="fréquence", col="green")
plot(PTRATIO,MEDV,xlab="ratio élèves-enseignants",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et ratio élèves-enseignants")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(B,main=NULL,xlab="proportion de population afro-américaine",ylab="fréquence", col="green")
plot(B,MEDV,xlab="proportion de population afro-américaine",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et proportion de population afro-américaine")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(LSTAT,main=NULL,xlab="proportion de population précaire",ylab="fréquence", col="green")
plot(LSTAT,MEDV,xlab="proportion de population précaire",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et proportion de population précaire")

par(mfrow = c(1, 2))
RADf=factor(RAD)
boxplot(MEDV~RADf,xlab="indice d'accessibilité aux autoroutes",ylab="valeur médiane du logement (K$)", col="green")
abline(h=mean(MEDV), col="red")
CHASf=factor(CHAS)
boxplot(MEDV~CHASf,xlab="proximité avec la rivière Charles",ylab="valeur médiane du logement (K$)", col="green")
abline(h=mean(MEDV), col="red")
```

### Annexe 1.2 : corrélation entre les variables du jeu de données
Les corrélations les plus significatives apparaissent en rouge.
```{r cor}
cordBH=round(cor(dHB,use="complete.obs"),2)
kable(ifelse(abs(cordBH)> 0.5, cell_spec(cordBH, "html", color = "red", 
    bold = T), cell_spec(cordBH, "html", color = "black")),format="markdown",digits=2)

```

### Annexe 1.3 : regression lineaire MEDV sur l'ensemble des variables
```{r modDM}
reg1=lm(MEDV~.,data=dHB)
summary(reg1)

```

### Annexe 1.4 : regression linéaire MEDV sur variables choisies par stepAIC
```{r stepwise}
dHB2 <-na.omit(dHB)
model <- lm(MEDV~.,data=dHB2) %>% stepAIC(trace = FALSE)
summary(model)

```

### Annexe 3.1 : traitement des DM par suppression
```{r modimpreg}
mu = colMeans(dHB, na.rm = TRUE)
cv = cov(dHB, use = "pairwise")

impreg= lavaan("MEDV~1 + CRIM + ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT
              MEDV ~~ MEDV",
             sample.mean = mu, sample.cov = cv,
             sample.nobs = sum(complete.cases(dHB)))
summary(impreg)
```

### Annexe 3.2 : Imputation - méthodologie
L'imputation correspond à l'action de convertir un échantillon incomplet en un échantillon complet. Le but de l'imputation multiple est d'affecter plusieurs fois des données manquantes, d'analyser les données complétées et ensuite d'intégrer les résultats des analyses.

Les 7 étapes de l'imputation:

**Etape 1** - Décider si supposition de MAR est plausible.  
(vu en partie 2)

**Etape 2** - Identifier la forme du modèle d'imputation.  
Le choix sera orienté par l’échelle de la variable à imputer, et intègre de préférence la connaissance de la relation entre les variables. L'algorithm MICE a besoin d'avoir une méthode univariée d'imputation pour chaque variable incomplète. 

**Etape 3** - Sélectionner le groupe de variables à inclure comme predicteurs dans le modèle d'imputation (fonction **mice**) 

```{r pred_matrix}
imp = mice(dHB, print = FALSE)
kable((imp$predictorMatrix),format="markdown")

```

Selon la matrice de résultat, CRIM sera prédit à partir de toutes les autres variables (indicateur = 1); idem pour ZN, INDUS, CHAS, AGE et LSTAT. Nous allons utiliser toutes les variables comme predicteurs. Cela est possible car le dataset est encore de taille raisonnable, (difficile sur les grands datasets , à cause de la multicolinearité ou de la capacité des machines) 

**Etape 4** - Imputer ou non des variables qui sont des fonctions d'autres variables incomplètes.     
Dans le cas de notre dataset, les variables avec des données manquantes ne sont pas des fonctions d'autres variables du dataset. Chacune répresente une thématique différente, utile pour l'estimation de la valeur de la maison.

**Etape 5** - Définir l'ordre d'imputation des variables (influe sur la convergence de l'algorithme). Par défaut, algorithme MICE impute les données incomplètes du dataset de gauche à droite. L'ordre est à changer si nous avons des soucis de convergence des algorithmes. 

**Etape 6** - Définir les imputations de départ et le nombre d'itérations 

**Etape 7** - Imputer et ajuster le modèle
la taille de m, le nombre d'ensembles de données imputées. L'imputation du dataset demande de faire des "essais-erreur", pour adapter et améliorer le modèle. Pour démarrer il est conseillé de mettre m = 5 et l'augmenter lors de la dernière étape si on est déjà satisfait avec le modèle.

### Annexe 3.3 : Imputation par régression stochastique
```{r modimpregstoed}
summary(dHBcompl.regsto)
regregsto=lm(MEDV~.-INDUS-AGE,data=dHBcompl.regsto)
summary(regregsto)
```

## Annexe 4.1 Diagnostic 2 : convergence des algorithmes. 

```{r diagnostics2}
plot(impregsto, main="Régression Stochastique", layout = c(2, 3))
plot(impforest, main="Forêts aléatoires", layout = c(2, 3))
plot(imppmm, main="Predictive mean matching", layout = c(2, 3))
#plot(imppca, main="ACP", layout = c(2, 3))
```



