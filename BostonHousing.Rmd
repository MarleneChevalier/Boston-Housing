---
title: "Projet : Méthodes de traitement de données manquantes"
author: "Olga Silva / Marlène Chevalier"
date: "30/11/2019"  
output:
  html_document: default
---

<style type="text/css">
body{ /* Normal  */
font-size: 12px;
}
td {  /* Table  */
font-size: 12px;
}
h1.title {
font-size: 26px;
}

h1 { /* Header 1 */
font-size: 20px;
}
h2 { /* Header 2 */
font-size: 16px;
font-weight:bold;
}
h3 { /* Header 3 */
font-size: 14px;
font-weight:bold;
}
</style>



```{r setup, include=FALSE}

#packages utilisés
library(knitr)
library(missMDA)
library(VIM)
library(MASS)
library(stats)

library(plotly)
library(fitdistrplus)
library(FactoMineR)
library(mice)
library(miceadds)
library(lavaan)
library(kableExtra)
library(dplyr)
library(missForest)
library(Amelia)

# options globales
opts_chunk$set(echo=FALSE)

# chargement des données
dHB=read.table("HousingData.csv",sep=",",header=TRUE)
attach(dHB)
```

# Sujet : Valeur du logement en banlieue de Boston 

Il s'agit de traiter les données manquantes du fichier Boston Housing. Ce fichier décrit la situation des logements dans les villes de la banlieue de Boston. Il est constitué de 506 enregistrements et de 14 variables quantitatives (soit 7084 données) :

 - **CRIM** : Taux de criminalité par habitant.   
 - **ZN** : Proportion de terrains résidentiels pour des lots de plus de 25000 pieds carré (environ 2300m²).   
 - **INDUS** : Proportion d'espace, en acres, consacré aux affaires non commerciales (1 acre environ 4000m²).   
 - **CHAS** : Proximité avec la rivière Charles (=1 si en bord de rivière / =0 si éloigné de la rivière) 
  - **NOX** : Concentration en oxyde d'azote (1 pour 10 millions)  
  - **RM** : Nombre moyen de chambres par logement  
  - **AGE** : Proportion des propriétés construites avant 1940
  - **DIS** : Moyenne des distances aux 5 centres d'emploi de Boston  
  - **RAD** : Indice d'accessibilité aux autoroutes (de 1 à 8 et 24)  
  - **TAX** : Taux d'imposition foncier (1 pour 10 000$)  
  - **PTRATIO** : Ratio d'élèves-enseignants  
  - **B** : Proportion de population afro-américaine
  - **LSTAT** : Proportion de population précaire 
  - **MDEV** : Valeur médiane des habitations privées (en K$) 
  
Nous utiliserons ces données pour tenter d'expliquer la valeur médiane des habitations privées (MDEV) en fonction des autres variables du fichier.
  
# 1.Exploration des données incomplètes

## Graphiques sur les données incomplètes

```{r stat}
moyMEDV=round(mean(MEDV),1)
sdMEDV=round(sd(MEDV),1)
minMEDV=min(MEDV)
maxMEDV=max(MEDV)

``` 
La valeur médiane du logement à Boston est comprise entre `r minMEDV` K$ et `r maxMEDV` K$ , en moyenne de `r moyMEDV` K$.

```{r expl}

par(mfrow = c(1, 1), pch = 16, col="blue")
hist(MEDV,main=NULL,xlab="valeur médiane du logement(K$)", ylab="fréquence", col="blue")
abline(v=mean(MEDV), col="red")

```

Graphiquement (*cf.annexe 1.1*), on observe que :   
   
   - Le **taux de criminalité** faible (inféreur à 10%) est le plus fréquent. La valeur du logement a tendance a diminué lorsque le taux de criminalité augmente. Mais la corrélation entre les 2 reste faible (0.4).  
   - La **proportion de terrains résidentiels** est majoritairement faible (inféreur à 10%), mais lorsqu'elle augmente, la valeur du logement a tendance a augmenté.  
   - La **proportion de surface d'activité industrielle** la plus fréquente est entre 18 et 20 acres (entre 72000m² et 80000m²). A ce nivea, la valeur moyenne est bien souvent inférieure à la valeur médiane moyenne des logements (22.5K$).  
   - La **concentration d'oxyde d'*azote** est le plus souvent entre 0.4 et 0.6. Plus la concentration augmente, plus la valeur des logements diminue.  
   - Le **nombre moyen de chambre** est le plus souvent entre 5 et 7. Plus le nombre de chambre augmente, plus les logements ont de la valeur.   
   - La **proportion de propriétés construites avant 1940** est très importante (majoritairement autour de 90% ). Plus cette proportion augmente, plus les logements ont de la valeur.  
   - La **moyenne des distances aux centres d'emploi** est fréquemment faible (<4). Cette variable influence peu la valeur des logements (corrélation=0.28).  
   - L'**imposition foncière** prend la plus importante partie de ses valeurs entre 200 et 500. Puis une autre serie importante de ses valeurs est autour de 666 ; à ce niveau d'impôt, la valeur du logement est plus faible (<moyenne 22.5).  
   - Le **ratio élèves-enseignants** est réparti quasi-équitablement autour de la valeur moyenne du logement.Une hausse de ce ratio a tendance à faire baisser le prix du logenment (corrélation =-0.54)
   - La **proportion de population afro-américaine** est importante; mais son influence n'est pas significative sur la valeur du logement (cor =0.35)
   - La **proportion de population précaire** influence négativement la valeur des logements (cor =-0.74).
   - L'**indice d'accessibilité aux autoroutes** à 24 donne les valeurs des logements les plus basses (15K$ en moyenne) . Les indices 3 et 8 donnent les valeurs de logement les plus élévées.  
   - La **proximité avec la rivière Charles** augmente légèrement la valeur du logement.
    
## Corrélation des variables

Les corrélations les plus significatives de la valeur du logement sont avec :  

   - RM (0.72) : plus le nombre de chambre est important, plus la valeur du biens est forte.  
   - INDUS et RAD (-0.51) : plus l'espace d'affaires non commerciales ou plus l'indice d'accessibilité aux autoroutes seront importants, moins la valeur du bien sera élevée.    
   - PTRATIO (-0.54) : plus le ratio elèves-enseignants est fort, moins la valeur des biens est élevée.  
   - LSTAT (-0.74) : une forte proportion de population précaire réduira très fortement la valeur des biens.  
(*cf. annexe 1.2*)

## Modélisation sur les données incomplètes
Nous commençons par examiner les résultats d'une regression linéaire de MEDV sur les autres variables. Le modèle est correctement ajusté (R²=0.75) mais il semble que les variables explicatives INDUS et AGE ne soient pas pertinentes pour ce modèle. (*cf. annexe 1.3*)

Nous voudrions maintenant utiliser la méthode "stepwise" pour choisir le meilleur modèle. Cependant, cette méthode ne fonctionne pas s'il y a des données manquantes. 
Afin de pouvoir l'utiliser, nous allons appliquer une méthode "listwise deletion" avec le risque de perte d'information et d'introduction de biais au dataset.   
Ce modèle écarte les variables INDUS et AGE, comme l'original, avec un R2 ajusté très proche de l'original. (*cf. annexe 1.4*)

**En conclusion** 

Nous allons comparer maintenant ce modèle obtenu avec des données manquantes et les nouveaux modèles que nous allons obtenir avec des datasets complets à partir des differents méthodes. 

(travail en cours)


# 2.Inventaire des données manquantes 

## Structure des données manquantes

Nous allons utiliser la fonction **md.pattern** du package mice pour analyser les données manquantes. Le résultat est une matrice, dans laquelle chaque ligne correspond à des structures de données manquantes et chaque colonne à une variable du fichier. Les lignes et les colonnes sont triées selon le niveau de complétude des données.   
A chaque ligne de la matrice (qui définit une structure de données manquantes du jeu de données) :  

  - la première colonne indique le nombre d'observations correspondant à la structure de données manquantes décrite ;    
  - la derniere colonne donne le nombre de variables incomplètes.

```{r inventDM1}
respattern=md.pattern(dHB,rotate.names = TRUE)
```

Au total, 120 observations sont manquantes : 20 pour chacune des variables CRIM, ZN, INDUS, CHAS, AGE, LSTAT. 

  - structure 1 : 394 observations pour lesquelles aucune donnée n'est manquante,
  - structure 2 : 18 observations pour lesquelles seule la donnée de la variable LSTAT est absente,
  - structure 3 : 18 observations pour lesquelles seule la donnée de la variable AGE est absente,   
  ...
  - structure 14 (dernière) : 1 observation pour laquelle les données des variables CRIM et CHAS sont absentes.
 
 (travail en cours) 
 
```{r inventDM3}
matrixplot(dHB,sortby=1)
```
 
## Proportion de données manquantes

La fonction **aggr** permet d'appréhender les données par leur proportion dans le jeucomplet.

```{r inventDM2}
aggr(dHB,numbers=TRUE,bars=FALSE)
```

En sortie, 2 graphiques :  

  - Le graphique de gauche donne la proportion de données maquantes de chaque variable : ici on retrouve des proportions égales pour les variables CRIM, ZN, INDUS, CHAS, LSTAT (autour de 4%), les autres variables sont complètes.    
  - Le graphique de droite donne la proportion de chaque structure de données.   
Ici 78% du jeux de données est complet pour toutes les variables, 3.8% des individus ont uniquement la variable CRIM qui n'est pas renseignée, ...

## Lien variables manquantes / variables observées

Le lien entre les variables maquantes et observées peut se mesurer par le tableau des "influx" et "outflux": fonction **flux**. Ces 2 statististiques permettent de quantifier la connexion entre les données observées et les données manquantes. 

L'**influx d'une variable** dépend de la proportion de données manquantes de la variable. L'influx d'une variable complètement observée est égal à 0, alors que pour les variables complètement manquantes, l'influx est à 1. A proportion de données maquantes équivalentes, la variable avec le plus fort influx sera la mieux connectée aux données observées et sera donc la plus facile à imputer.

L'**outflux d'une variable** indique l'importance des valeurs observées pour imputer les valeur manquantes. L'outflux d'une variable complètement observée est égal à 1, tandis que l'outflux d'une variable complètement manquante est égal à 0. Pour deux variables ayant la même proportion de données manquantes, la variable avec un ouflux plus élevé est mieux connectée aux données manquantes, et donc potentiellement plus utile pour imputer d'autres variables.  
(*cf. annexe 2.1*)

Ici, les variables CRIM, ZN, INDUS, CHAS, AGE, LSTAT ont un influx supérieur à 0 (0.04) et un outflux est inférieur à 1 (0.8), ce qui confirme qu'elles sont incomplètes. Les autres variables sont complétement renseignées (influx=0 et outflux=1).

## Catégories de données manquantes

Rubin (1976) a classé les problèmes des données manquantes en trois catégories :  

  - **Données manquantes de façon complètement aléatoire : MCAR** (missing completely at random). L'absence de  données est dûe au hasard, à la malchance. Cette hypothèse est peu réaliste.  
  - **Données manquantes de façon aléatoire : MAR** (missing at random). La probabilité d’absence de la valeur d’une variable dépend des valeurs prises par d’autres variables qui ont été observées. MAR est plus générale et plus réaliste que MCAR.
  - **Données manquantes de façon non aléatoire : MNAR ** (missing not at random). La cause d’absence de la valeur d’une variable est de raision inconnue. MNAR est le cas le plus complexe.

La plupart des méthodes modernes de traitement des données manquantes partent de la supposition MAR. Dans le cas du jeu de données Boston Housing, on peut aussi partir de cette hypothèse (il nous manque que 22% des données , donc ici nous sommes loin du point problèmatique du 50%)


**En conclusion**  

  - 6 variables sont incomplètes, avec chacune 20 données manquantes (soit 120 au total) :

     - **CRIM** : Taux de criminalité par habitant     
     - **ZN** : Proportion de terrains résidentiels  
     - **INDUS** : Proportion d'espace consacré aux affaires non commerciales  
     - **CHAS** : Proximité avec la rivière Charles  
     - **AGE** : Proportion des propriétés construites avant 1940  
     - **LSTAT** : Proportion de population précaire  
   
Sur le jeu de données, 22% des individus sont incomplets  

  - Nous supposons qu'on est en situation **MAR (Données manquantes de façon aléatoire)**

# 3.Traitement des données manquantes

**Complete Cases**

Une méthode de traitement les données manquantes peut être la suppression des lignes avec des valeurs absentes, pour avoir un dataset complet. La suppression par paire est une alternative à la suppression simple des lignes avec des données manquantes. Avec cette méthode, il faut calculer la moyenne, variance et covariance de toutes les données observées.

Cette méthode pourrait poser des problèmes sur les variables très corrélées entre elles et si les données ne suivent pas une distribution normale. 

```{r modimpreg}
mu = colMeans(dHB, na.rm = TRUE)
cv = cov(dHB, use = "pairwise")

impreg= lavaan("MEDV~1 + CRIM + ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT
              MEDV ~~ MEDV",
             sample.mean = mu, sample.cov = cv,
             sample.nobs = sum(complete.cases(dHB)))
summary(impreg)
```

Nous observons que cette méthode, comme prévu, va supprimer les lignes avec des données manquantes, donc le dataframe d'analyse a uniquement 394 lignes. Selon les résultats, INDUS et AGE peuvent sortir du modèle, elles ne sont pas representatives. 

Par contre c'est une méthode à eviter, car nous allons perdre des informations et nous pouvons ajouter des biais au modèle. 
Nous preferons continuer notre recherche sur les méthodes d'imputation.

**L'imputation**

L'imputation correspond à l'action de convertir un échantillon non complet en un échantillon complet. Le but de l'imputation multiple est d'affecter plusieurs fois des données manquantes, d'analyser les données complétées et ensuite d'intégrer les résultats des analyses.

Nous avons 7 choix à faire pour specifier le modèle à utiliser pour l'imputation des données :

1 - Décider si supposition de MAR est plausible.  
(vu en partie 2)

2 - La forme du modèle d'imputation.  
Le choix sera orienté par l’échelle de la variable à imputer, et intègre de préférence la connaissance de la relation entre les variables. L'algorithm MICE a besoin d'avoir un méthode univarié d'imputation pour chaque variable incomplète. 

3 - Le groupe de variables à inclure comme predicteurs dans le modèle d'imputation. 


```{r pred_matrix}
imp = mice(dHB, print = FALSE)
kable((imp$predictorMatrix),format="markdown")

```

Selon la matrice de résultat, CRIM sera prédit à partir de toutes les autres variables; pareil pour ZN, INDUS, CHAS, AGE et LSTAT. Nous allons utiliser toutes les variables comme predicteurs. Cela est possible car le dataset est encore de taille raisonable, pour des grands datasets cela pourrait ne pas être possible, à cause de la multicolinearité ou de la capacité des machines. 

4 - Imputer ou non des variables qui sont des fonctions d'autres variables incomplètes.     
Dans le cas de notre dataset, les variables avec des données manquantes ne sont pas des fonctions d'autres variables du dataset. Chacune répresente une thématique différente, utile pour l'estimation de la valeur de la maison.

5 - L'ordre d'imputation des variables (influe sur la convergence de l'algorithme).    
Par défaut, algorithme MICE impute les données incomplètes du dataset de gauche à droite. 


```{r convergence}
imp = mice(dHB, seed = 62006, maxit = 20, print = FALSE)
plot(imp)

```

Voici les graphiques des variations de la moyenne et l'écart type, avec la méthode pmm, pour chaque itération et chaque donnée imputée. Pour la convergence, les différentes lignes doivent se mélanger librement sans une tendance définitive. C'est bien le cas pour nos graphiques, donc, nous n'avons pas des problèmes de convergence. Cette convergence sera testé pour chaque modèle.

6 - La mise en place des imputations de départ et le nombre d'itérations.  

7 - la taille de m, le nombre d'ensembles de données imputées. L'imputation du dataset demande de faire des "essais-erreur", pour adapter et améliorer le modèle. Pour démarrer il est conseillé de mettre m = 5 et l'augmenter lors de la dernière étape si on est déjà satisfait avec le modèle.


## Imputation simple

Les méthodes d'imputation simples consistent à remplacer chaque valeur maquante par une donnée prédite ou simulée. 
Plusieurs solutions sont possibles (remplacer les données manquantes par la moyenne de la variable, faire une régression avec les données observées...). Ces solutions rapides sont à éviter car cela modifie la corrélation entre les variables, la distribution, la variable est sous-estimée, entre autres problèmes. 

Nous allons ici en tester 3 : l'imputation par regression stochastique, le random forest et predictive mean matching. Pour les trois méthodes nous allons les appliquer à des modèles sans INDUS et AGE, car elles ne semblent pas pertinentes, selon les résultats précèdents.  


### Imputation par regression stochastique

Cette méthode consiste à imputer les données manquantes en utilisant la regression à laquelle on a ajouté du bruit. Cela permet de corriger le biais de corrélation qui existe par les méthodes plus rapides (décrites plus haut).

```{r modimpregsto}
impregsto = mice(dHB, method = "norm.nob", m = 6, maxit = 60,
            seed = 600, print=FALSE)
dHBcompl.regsto=complete(impregsto)
summary(dHBcompl.regsto)

##regregsto1=lm(MEDV~.,data=dHBcompl.regsto)
##summary(regregsto1)
regregsto2=lm(MEDV~.-INDUS-AGE,data=dHBcompl.regsto)
summary(regregsto2)


```

Avec cette méthode on observe un R2 ajustée de 0.736. Proche mais plus bas que le R2 ajustée de la regression faite avec les données manquantes.

### Imputation par forêts aléatoires

Avec cette méthode, pour les variables continues les valeurs son imputées en faisant des tirages aléatoires à partir des distributions gaussiens independants, centrées en les moyennes prédites par les forêts aléatoires.


```{r modimpforest, warning=FALSE}

impforest = mice(dHB, method = "rf", m = 5, seed = 600, print=FALSE)

dHBcompl.rf=complete(impforest)
summary(dHBcompl.rf)
##regregsto1=lm(MEDV~.,data=dHBcompl.regsto)
##summary(regregsto1)
regrf=lm(MEDV~.-INDUS-AGE,data=dHBcompl.rf)
summary(regrf)


```

Avec cette méthode on observe un R2 ajustée de 0.734. Proche mais plus bas que le R2 ajustée de la regression faite avec les données manquantes. Les valeus estimées restent aussi très proches.

### Imputation par predictive mean matching

La méthode pmm est très répandue car elle est facile à utiliser, elle sélectionne un petit ensemble des "donneurs" à partir des cas complets, avec des valeurs prédits proches des valeurs prédits par les données manquantes. Un donneur est pris aléatoirement et la valeur observée remplace la donnée manquante. 

```{r modpmm, warning=FALSE}

imppmm = mice(dHB, method = "pmm", m = 5, seed = 600, print=FALSE)

dHBcompl.pmm=complete(imppmm)
summary(dHBcompl.pmm)
##regregsto1=lm(MEDV~.,data=dHBcompl.regsto)
##summary(regregsto1)
regpmm=lm(MEDV~.-INDUS-AGE,data=dHBcompl.pmm)
summary(regpmm)


```

Pareil que pour les deux méthodes précedents, le R2 ajusté est très proche du modèle avec des données manquantes.


## Imputation multiple

L'imputation multiple consiste en trois phases :

1 - Imputation des données manquantes m fois
2 - Analyse de m datasets imputés
3 - Mise en commun des paramètres à travers m analyses


## Imputation par ACP

Nous alloms gerer les données manquants en utilisant la ACP (Analyse des composants principales), en generant plusieurs tableaux completés.

** tu avais fait la PCA avec des données manquantes, mais non l'imputation par ACP**
```{r pca}
nb = estim_ncpPCA(dHB,ncp.min=0,ncp.max=6)
resMIPCA <- MIPCA(dHB,ncp=2, nboot=100)
plot(resMIPCA,choice= "var")

```

Ce graphique represente la projection des individus pour chaque dataset imputé, avec l'algorithme iterative de l'ACP. Nous voyoms au'il y a très peu d'incertitude sur les variables (les nuages des points sont très rapprochés)

Regression linéaire avec ACP

```{r lm_pca}
imp.mice <- mice(dHB, m=10,defaultMethod="norm.boot") 
lm.mice.out <- with(imp.mice, lm(MEDV~CRIM+ZN+CHAS+NOX+RM+DIS+RAD+TAX+PTRATIO+B+LSTAT))

resMIPCA <- lapply(resMIPCA$res.MI, as.data.frame)
fitMIPCA<-lapply(resMIPCA,lm, formula="MEDV~CRIM+ZN+CHAS+NOX+RM+DIS+RAD+TAX+PTRATIO+B+LSTAT")
poolMIPCA<-pool(as.mira(fitMIPCA))
summary(pool(fitMIPCA))

```


## Critères d'évaluation de la qualité de l'imputation

  1 - Raw bias (RB) and percent bias (PB). Le biais brut de Q est defini comme la différence entre l'espérance de l'estimateur et le réél. Il doit être proche de zéro.    
 
  2 - Coverage rate (CR). c'est la proportion des intervalles de confiance qui contiennent la valeur réelle. Il doit être proche de 0.95.  
  
  3 - Average width (AW), c'est un indicateur de l'efficacité statistique. La largeur doit être le plus petite possible, sans que le CR soit plus bas que la valeur nominale.   
Des méthodes sans biais et avec un bon CR sont connues comme "randomization-valid" (Rubin 1987b). 

**Graphiques de diagnostic**

Ici il faut comparer les graphiques de densite des variables + convergence des modeles avant de selectionner le meilleur (en cours)
```{r plots_mice}
##densityplot(imppmm, layout = c(3, 1))
##overimpute(res.amelia, var = "maxO3")
##compare.density(res.amelia, var = "T12")
```

# Conclusion


# Annexes  

### Annexe 1.1 : graphiques exploratoires du jeu de données
Les representations graphiques présentent, pour chaque variable explicative, la distribution sous forme d'histogramme et le nuage de point de la variable vs la valeur médiane du logement (en rouge la valeur moyenne = 22.5K$).


```{r angraDM}

par(mfrow = c(1, 2), pch = 16, col="green")
hist(CRIM,main=NULL,xlab="taux de criminalité",ylab="fréquence", col="green")
plot(CRIM,MEDV,xlab="taux de criminalité",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et taux de criminalité")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(ZN,main=NULL,xlab="prop.terrains résidentiels",ylab="fréquence", col="green")
plot(ZN,MEDV,xlab="prop.surface activité industrielle",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title(main="Valeur médiane du logement et proportion de terrains résidentiels")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(INDUS,main=NULL,xlab="prop.surface activité industrielle",ylab="fréquence", col="green")
plot(INDUS,MEDV,xlab="prop.surface activité industrielle",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title(main="Valeur médiane du logement et proportion de surface d'activité industrielle")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(NOX,main=NULL,xlab="concentration oxyde d'azote (1/10M)",ylab="fréquence", col="green")
plot(NOX,MEDV,xlab="concentration oxyde d'azote (1/10M)",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et concentration en oxyde d'azote")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(RM,main=NULL,xlab="nombre de chambre",ylab="fréquence", col="green")
plot(RM,MEDV,xlab="nombre de chambre",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et nombre moyen de chambre")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(AGE,main=NULL,xlab="prop. construites avant 1940",ylab="fréquence", col="green")
plot(AGE,MEDV,xlab="prop. construites avant 1940",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et proportion de propriétes construites avant 1940")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(DIS,main=NULL,xlab="distance moyenne aux centres d'emplois",ylab="fréquence", col="green")
plot(DIS,MEDV,xlab="distance moyenne aux centres d'emplois",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et distance moyenne aux 5 centres d'emplois")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(TAX,main=NULL,xlab="taux d'imposition foncier (1/10K$",ylab="fréquence", col="green")
plot(TAX,MEDV,xlab="taux d'imposition foncier (1/10K$",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et taux d'imposition foncier")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(PTRATIO,main=NULL,xlab="ratio élèves-enseignants",ylab="fréquence", col="green")
plot(PTRATIO,MEDV,xlab="ratio élèves-enseignants",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et ratio élèves-enseignants")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(B,main=NULL,xlab="proportion de population afro-américaine",ylab="fréquence", col="green")
plot(B,MEDV,xlab="proportion de population afro-américaine",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et proportion de population afro-américaine")

par(mfrow = c(1, 2), pch = 16, col="green")
hist(LSTAT,main=NULL,xlab="proportion de population précaire",ylab="fréquence", col="green")
plot(LSTAT,MEDV,xlab="proportion de population précaire",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et proportion de population précaire")

par(mfrow = c(1, 2))
RADf=factor(RAD)
boxplot(MEDV~RADf,xlab="indice d'accessibilité aux autoroutes",ylab="valeur médiane du logement (K$)", col="green")
abline(h=mean(MEDV), col="red")
CHASf=factor(CHAS)
boxplot(MEDV~CHASf,xlab="proximité avec la rivière Charles",ylab="valeur médiane du logement (K$)", col="green")
abline(h=mean(MEDV), col="red")
```

### Annexe 1.2 : corrélation entre les variables du jeu de données
Les corrélations les plus significatives apparaissent en rouge.
```{r cor}
cordBH=round(cor(dHB,use="complete.obs"),2)
kable(ifelse(abs(cordBH)> 0.5, cell_spec(cordBH, "html", color = "red", 
    bold = T), cell_spec(cordBH, "html", color = "black")),format="markdown",digits=2)

```

### Annexe 1.3 : regression lineaire MEDV sur l'ensemble des variables
```{r modDM}
reg1=lm(MEDV~.,data=dHB)
summary(reg1)

```
### Annexe 1.4 : regression linéaire MEDV sur variables choisies par stepwise

```{r stepwise}
dHB2 <-na.omit(dHB)
model <- lm(MEDV~.,data=dHB2) %>% stepAIC(trace = FALSE)
summary(model)

```
### Annexe 2.1 : lien entre les données manquantes et observées

```{r DMflux}
fluxdBH=round(as.matrix(flux(dHB)[,2:3]),2)
#kable((fluxdBH),format="markdown",digits=3)
kable(ifelse(((fluxdBH)<1 & (fluxdBH)>0),  cell_spec(fluxdBH, "html", color = "red", bold = T), cell_spec(fluxdBH, "html", color = "black", bold = T)), format="markdown")
```

