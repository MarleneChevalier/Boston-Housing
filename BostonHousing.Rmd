---
title: 'Projet : Méthodes de traitement de données manquantes'
author: "Olga Silva / Marlène Chevalier"
date: "30/11/2019"
output:
  html_document: default
  pdf_document: default
---

<style type="text/css">
body{ /* Normal  */
font-size: 14px;
}
td {  /* Table  */
font-size: 14px;
}
h1.title {
font-size: 28px;
}

h1 { /* Header 1 */
font-size: 22px;
}
h2 { /* Header 2 */
font-size: 18px;
font-weight:bold;
}
h3 { /* Header 3 */
font-size: 16px;
font-weight:bold;
}
</style>



```{r setup, warning=FALSE, include=FALSE}

#packages utilisés
library(knitr)
library(missMDA)
library(VIM)
library(MASS)
library(stats)
library(micemd)

library(plotly)
library(fitdistrplus)
library(FactoMineR)
library(mice)
library(miceadds)
library(lavaan)
library(kableExtra)
library(dplyr)
library(missForest)

# options globales
opts_chunk$set(echo=FALSE,warning=FALSE)

# parametrage des graphiques
theme_set=(theme_classic()+theme(plot.title=element_text(hjust=0.5,size=14,face="bold"),plot.subtitle=element_text(hjust=0.5,size=12) ,axis.title=element_text(size=10)))  

# chargement des données
dHB=read.table("HousingData.csv",sep=",",header=TRUE)
attach(dHB)
```

# Sujet : Valeur du logement en banlieue de Boston 

Il s'agit de traiter les données manquantes du fichier Boston Housing. Ce fichier décrit la situation des logements dans les villes de la banlieue de Boston. Il est constitué de 506 enregistrements et de 14 variables quantitatives (soit 7084 données) :

 - **CRIM** : Taux de criminalité par habitant.   
 - **ZN** : Proportion de terrains résidentiels pour des lots de plus de 25000 pieds carré (environ 2300m²).   
 - **INDUS** : Proportion d'espace, en acres, consacré aux affaires non commerciales (1 acre environ 4000m²).   
 - **CHAS** : Proximité avec la rivière Charles (=1 si en bord de rivière / =0 si éloigné de la rivière) 
  - **NOX** : Concentration en oxyde d'azote (1 pour 10 millions)  
  - **RM** : Nombre moyen de chambres par logement  
  - **AGE** : Proportion des propriétés construites avant 1940
  - **DIS** : Moyenne des distances aux 5 centres d'emploi de Boston  
  - **RAD** : Indice d'accessibilité aux autoroutes (de 1 à 8 et 24)  
  - **TAX** : Taux d'imposition foncier (1 pour 10 000$)  
  - **PTRATIO** : Ratio d'élèves-enseignants  
  - **B** : Proportion de population afro-américaine
  - **LSTAT** : Proportion de population précaire 
  - **MDEV** : Valeur médiane des habitations privées (en K$) 
  
Nous utiliserons ces données pour tenter d'expliquer la valeur médiane des habitations privées (MDEV) en fonction des autres variables du fichier.
  
# 1.Exploration des données incomplètes

## Graphiques sur les données incomplètes

```{r stat}
moyMEDV=round(mean(MEDV),1)
sdMEDV=round(sd(MEDV),1)
minMEDV=min(MEDV)
maxMEDV=max(MEDV)

``` 
La valeur médiane du logement à Boston est comprise entre `r minMEDV` K$ et `r maxMEDV` K$ , en moyenne de `r moyMEDV` K$.

```{r expl, fig.height = 3.5, fig.width = 5}
##par(mfrow = c(1, 1), pch = 16, col="blue")
hist(MEDV,main=NULL,xlab="valeur médiane du logement(K$)", ylab="fréquence", col="blue")
abline(v=mean(MEDV), col="red")

```

Graphiquement (*cf.annexe 1.2*), on observe que :   
   
   - Le **taux de criminalité** faible (inféreur à 10%) est le plus fréquent. La valeur du logement a tendance a diminué lorsque le taux de criminalité augmente. Mais la corrélation entre les 2 reste faible (0.4).  
   - La **proportion de terrains résidentiels** est majoritairement faible (inféreur à 10%), mais lorsqu'elle augmente, la valeur du logement a tendance a augmenté.  
   - La **proportion de surface d'activité industrielle** la plus fréquente est entre 18 et 20 acres (entre 72 000m² et 80 000m²). A ce niveau, la valeur moyenne est bien souvent inférieure à la valeur médiane moyenne des logements (22.5K$).  
   - La **concentration d'oxyde d'*azote** est le plus souvent entre 0.4 et 0.6. Plus la concentration augmente, plus la valeur des logements diminue.  
   - Le **nombre moyen de chambre** est le plus souvent entre 5 et 7. Plus le nombre de chambre augmente, plus les logements ont de la valeur.   
   - La **proportion de propriétés construites avant 1940** est très importante (majoritairement autour de 90% ). Plus cette proportion augmente, plus les logements ont de la valeur.  
   - La **moyenne des distances aux centres d'emploi** est fréquemment faible (<4). Cette variable influence peu la valeur des logements (corrélation=0.28).  
   - L'**imposition foncière** prend la plus importante partie de ses valeurs entre 200 et 500. Puis une autre série importante de ses valeurs est autour de 666 ; à ce niveau d'impôt, la valeur du logement est plus faible (<moyenne 22.5).  
   - Le **ratio élèves-enseignants** est réparti quasi-équitablement autour de la valeur moyenne du logement.Une hausse de ce ratio a tendance à faire baisser le prix du logenment (corrélation =-0.54)
   - La **proportion de population afro-américaine** est importante; mais son influence n'est pas significative sur la valeur du logement (cor =0.35)
   - La **proportion de population précaire** influence négativement la valeur des logements (cor =-0.74).
   - L'**indice d'accessibilité aux autoroutes** à 24 donne les valeurs des logements les plus basses (15K$ en moyenne) . Les indices 3 et 8 donnent les valeurs de logement les plus élevées.  
   - La **proximité avec la rivière Charles** augmente légèrement la valeur du logement.
    
## Corrélation des variables

Les corrélations les plus significatives de la valeur du logement sont avec :  

   - RM (0.72) : plus le nombre de chambre est important, plus la valeur du biens est forte.  
   - INDUS et RAD (-0.51) : plus l'espace d'affaires non commerciales ou plus l'indice d'accessibilité aux autoroutes seront importants, moins la valeur du bien sera élevée.    
   - PTRATIO (-0.54) : plus le ratio elèves-enseignants est fort, moins la valeur des biens est élevée.  
   - LSTAT (-0.74) : une forte proportion de population précaire réduira très fortement la valeur des biens.  
(*cf. annexe 1.3*)

## Modélisation sur les données incomplètes

Nous commençons par examiner les résultats d'une regression linéaire de MEDV sur les autres variables. Le modèle est correctement ajusté (R²ajusté=0.7591) mais il semble que les variables explicatives INDUS et AGE ne soient pas pertinentes pour ce modèle. (**cf. annexe 1.4**)

Nous allons appliquer une méthode "step AIC" pour choisir le meilleur modèle. Cependant pour utiliser cette méthode, nous devons supprimer les lignes avec des données manquantes, avec le risque de perte d'information et d'introduction de biais au dataset. Ce modèle écarte aussi les variables INDUS et AGE avec un R² ajusté très proche (0.7598) de l'original. (**cf. annexe 1.5**)

Cependant, en regardant le graphique des résidus (**cf. annexe 1.6**) nous observons que le modèle n'est pas optimal : il semble qu'il y ait une relation non linéaire avec une ou plusieurs variables. Pour la trouver, nous faisons un graphique par pairs (**cf. annexe 1.7**)  et observons une possible relation polynomiale d'ordre 2 entre LSTAT et MEDV. Si nous corrigeons notre modèle en ajoutant un ordre quadratique, le R² ajusté est légèrement meilleur (0.7965) et les résidus s'améliorent nettement. (**cf. annexe 1.8**)


## En conclusion sur l'exploration de données

La valeur médiane du logement à Boston et ses environs est comprise entre 5 et 50K dollars. Sa distribution est croissante jusqu'à sa valeur moyenne 22.5K dollars puis décroit fortement à partir de 25K dollars.  

**La valeur du logement est influencée ** (p_value <5%): 

  **- positivement** principalement (coefficient estimé = 3.58) **par le nombre de chambres** et plus faiblement (coefficients estimés de 0.25 et 0.03) **par l'accessibilité aux autoroutes et la proportion de terrain résidentiel**   
  
  **- négativement** principalement (coefficient estimé = -14.71) **par la concentration en oxyde d'azote** et plus faiblement (coefficients estimés entre de -1.34, -0.76, -1.45, -0.01) **par la distance aux centres d'emplois, le ratio élèves-enseignants, la proportion de population précaire et le taux foncier**   
Le R² ajusté du modèle de regression sur jeu de données incomplet est de **0.7965 (R² de référence)**.    
(**cf. annexe 8** : résultat de la regression linéaire après selection de variable)

Les variables INDUS et AGE ne sont pas significatives dans l'explication de la valeur du logement (p-value>5% **cf. annexe 1.4**), et LSTAT a une relation quadratique avec MEDV. 

**Le meilleur modèle sur dataset incomplet testé ici est la regression lineaire du prix médian du logement, MEDV, sur l'ensemble des variables explicatives, hormis INDUS et AGE et incluant LSTAT².**

Nous allons comparer maintenant le meilleur modèle obtenu avec des données manquantes et les nouveaux modèles que nous allons obtenir avec des datasets complets à partir de differentes méthodes. 


# 2.Inventaire des données manquantes 
Il s'agit ici d'identifier les données manquantes par variable, représenter leur structure dans le jeu de données.

## Structure des données manquantes

La fonction **md.pattern** du package MICE a pour résultat une matrice, dans laquelle chaque ligne correspond à des structures de données manquantes et chaque colonne à une variable du fichier. Les lignes et les colonnes sont triées selon le niveau de complétude des données.   
A chaque ligne de la matrice (qui définit une structure de données manquantes du jeu de données) :  

  - la première colonne indique le nombre d'observations correspondant à la structure de données manquantes décrite ;    
  - la derniere colonne donne le nombre de variables incomplètes.

```{r inventDM1, fig.height = 3.5, fig.width = 5}
respattern=md.pattern(dHB,rotate.names = TRUE)
```

Au total, 120 observations sont manquantes : 20 pour chacune des variables CRIM, ZN, INDUS, CHAS, AGE, LSTAT. 

  - structure 1 : 394 observations pour lesquelles aucune donnée n'est manquante,
  - structure 2 : 18 observations pour lesquelles seule la donnée de la variable LSTAT est absente,
  - structure 3 : 18 observations pour lesquelles seule la donnée de la variable AGE est absente,   
  ...
  - structure 14 (dernière) : 1 observation pour laquelle les données des variables CRIM et CHAS sont absentes.
 
Une deuxième representation des données manquantes obtenue avec **matrixplot** peut être observé en **annexe 2.1**. Elle permet d'identifier des dépendences des valeurs extrêmes et des données manquantes. Les données observées sont en gris et les manquantes en rouge. Nous pouvons observer que les valeurs manquantes se répartissent bien dans l'ensemble du jeu de données.
 
## Proportion de données manquantes

La fonction **aggr** permet d'appréhender les données par leur proportion dans le jeu complet.

```{r inventDM2, fig.height = 3.5}
aggr(dHB,numbers=TRUE,bars=FALSE)
```

En sortie, 2 graphiques :  

  - Le graphique de gauche donne la proportion de données manquantes de chaque variable : ici on retrouve des proportions égales pour les variables CRIM, ZN, INDUS, CHAS, LSTAT (autour de 4%), les autres variables sont complètes.    
  - Le graphique de droite donne la proportion de chaque structure de données.   

Ici 78% du jeu de données est complet pour toutes les variables, 3.8% des individus ont uniquement la variable CRIM qui n'est pas renseignée, ...

## Catégories de données manquantes

Rubin (1976) a classé les problèmes des données manquantes en trois catégories :  

  - **Données manquantes de façon complètement aléatoire : MCAR** (missing completely at random). L'absence de  données est dûe au hasard, à la malchance. Cette hypothèse est peu réaliste.  
  - **Données manquantes de façon aléatoire : MAR** (missing at random). La probabilité d’absence de la valeur d’une variable dépend des valeurs prises par d’autres variables qui ont été observées. MAR est plus générale et plus réaliste que MCAR.
  - **Données manquantes de façon non aléatoire : MNAR ** (missing not at random). La cause d’absence de la valeur d’une variable est de raision inconnue. MNAR est le cas le plus complexe.

La plupart des méthodes modernes de traitement des données manquantes partent de la supposition MAR. Dans le cas du jeu de données Boston Housing, par rapport à notre exploration initial, nous pouvons aussi partir de cette hypothèse.

## En conclusion sur l'inventaire des données manquantes 

  - 6 variables sont incomplètes, avec chacune 20 données manquantes (soit 120 au total) :

     - **CRIM** : Taux de criminalité par habitant     
     - **ZN** : Proportion de terrains résidentiels  
     - **INDUS** : Proportion d'espace consacré aux affaires non commerciales  
     - **CHAS** : Proximité avec la rivière Charles  
     - **AGE** : Proportion des propriétés construites avant 1940  
     - **LSTAT** : Proportion de population précaire  
   
Sur le jeu de données, 22% des individus sont incomplets  

  - Nous supposons qu'on est en situation **MAR (Données manquantes de façon aléatoire)**

# 3.Traitement des données manquantes

## Imputation multiple

Les méthodes d'imputation simple consistent à remplacer chaque valeur manquante par une valeur unique prédite ou simulée. Plusieurs solutions sont possibles : remplacer les données manquantes par la moyenne de la variable, faire une régression avec les données observées.... Ces solutions rapides sont à éviter car elle peuvent dégrader l'information (corrélation entre les variables, modification de la distribution des variables, variables sous-estimées ...). Nous allons utiliser uniquement l'imputation multiple pour notre projet.  

L'imputation multiple va créer m datasets complets, au lieu d'imputer une seule fois comme l'imputation simple. Les méthodes d'imputation multiple suivent trois grandes étapes :

  - **Etape 1** : Imputation des données manquantes m fois  
  - **Etape 2** : Analyse de m datasets imputés  
  - **Etape 3** : Mise en commun des paramètres à travers m analyses  

Plus de détails sur la méthodologie se trouve sur l'**annexe 3.1** 

Nous allons ici tester plusieurs méthodes d'imputation multiple : l'imputation par regression stochastique, les forêts aléatoires, predictive mean matching et l'ACP. Nous allons appliquer ces méthodes à des modèles sans INDUS et AGE, car ces variables ne semblent pas pertinentes (cf.résultats de la partie 1). 


### Imputation par régression stochastique

Cette méthode consiste à imputer les données manquantes en utilisant la regression à laquelle on a ajouté du bruit. Cela permet de corriger le biais de corrélation qui existe par les méthodes plus rapides d'imputation simple. Nous fixons m=6, en suivant les conseils de Stef Van Buuren dans son livre "flexible imputation data".

Pour faire cette imputation, nous utilisons la fonction **mice ( avec method = "norm.nob")**

```{r modimpregsto}
t0.regsto=Sys.time() 
impregsto = mice(dHB, method = "norm.nob", m = 6, maxit = 60,
            seed = 600, print=FALSE)
dHBcompl.regsto=complete(impregsto)
t1.regsto=Sys.time() 
tdiff.regsto= difftime(t1.regsto, t0.regsto, units="secs")

regregsto=lm(MEDV ~ CRIM + ZN + CHAS + NOX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT+ I(LSTAT^2),data=dHBcompl.regsto)
summary(regregsto)
regsto.adjr2=round(summary(regregsto)$adj.r.squared,2)
```

Avec cette méthode, on observe un R² ajusté de **`r regsto.adjr2`** après la régression linéaire faite avec le dataset completé. Sa valeur est proche de celle obtenue pourla régression faite avec les données manquantes.

La description de la nouvelle structure des données se trouve sur l'**annexe 3.2**. Sur cette description nous observons que pour deux variables (CRIM et LSTAT), l'imputation propose des valeurs négatives en sachant que sur le dataset d'origine ces deux variables sont toujours positives.

### Imputation par forêts aléatoires

Dans l'imputation par forêts aléatoire, les valeurs sont imputées en faisant des tirages aléatoires à partir des distributions gaussiennes independantes, centrées en les moyennes prédites par les forêts aléatoires. Pour ce faire , nous utilisons la fonction **mice ( avec method = "rf")**, avec m=6

```{r modimpforest, warning=FALSE}
t0.rf=Sys.time() 
impforest = mice(dHB, method = "rf", m = 6, seed = 600, print=FALSE)
dHBcompl.rf=complete(impforest)
t1.rf=Sys.time() 
tdiff.rf= difftime(t1.rf, t0.rf, units="secs")
regrf=lm(MEDV ~ CRIM + ZN + CHAS + NOX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT+ I(LSTAT^2),data=dHBcompl.rf)
summary(regrf)
regrf.adjr2=round(summary(regrf)$adj.r.squared,2)

```

Avec cette méthode on observe un R² ajusté de **`r regrf.adjr2`** avec le dataset completé, proche de celui obtenu avec la méthode d'imputation stochastique.  Les valeurs estimées et les résidus restent aussi très proches.

La description de la nouvelle structure des données se trouve sur l'**annexe 3.3**. Ici le nouveau dataset completé est cohérent avec celui d'origine.

### Imputation par predictive mean matching

La méthode pmm commence par faire une regression linéaire de variable pour estimer les paramètres de la distribution qui lie les variables avec données manquantes X et les variables complètes Y du dataset.  A l'aide de cette distribution, on simule des prédictions de valeurs manquantes et présentes de la variable X. Ensuite on identifie pour chaque donnée manquante de X un ensemble de prédiction sur les valeurs présentes de X, proches de la prédiction sur la valeur manquante. Parmi cet ensemble de cas proches, on en choisit un au hasard et on attribue alors la valeur observée pour remplacer la valeur manquante. 

Pour faire cette imputation, nous utilisons la fonction **mice ( avec method = "pmm")**, m=6

```{r modpmm}
t0.pmm=Sys.time() 
imppmm = mice(dHB, method = "pmm", m = 6, seed = 600, print=FALSE)
dHBcompl.pmm=complete(imppmm)
t1.pmm=Sys.time() 
tdiff.pmm= difftime(t1.pmm, t0.pmm, units="secs")

regpmm=lm(MEDV ~ CRIM + ZN + CHAS + NOX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT+ I(LSTAT^2),data=dHBcompl.pmm)
summary(regpmm)
regpmm.adjr2=round(summary(regrf)$adj.r.squared,2)

```

De même que pour les deux méthodes précédentes, le R² ajusté **`r regpmm.adjr2`** est très proche du modèle avec des données manquantes. Les valeurs estimées et les résidus restent aussi très proches.

La description de la nouvelle structure des données se trouve sur l'**annexe 3.4** et on observe que les valeurs mininimum, maximum et médianne sont presque idéntiques à celles obtenues avec les forêts aléatoires.

## Imputation à l'aide d'un traitement bayésien de l'ACP

Les données manquantes sont imputées en utilisant la ACP (Analyse des composantes principales).  
Il s'agit d'abord d'estimer le nombre de composantes utilisés pour compléter les données avec la méthode de ACP : fonction **estim_ncpPCA(dHB,method.cv = "Kfold")** 

```{r dim_pca1, include=FALSE}
t0.acp=Sys.time() 

nb.kfold = estim_ncpPCA(dHB,method.cv = "Kfold")
```

Selon le graphique (cf. **annexe 3.5**), le nombre de dimensions à retenir est de 3.

   - générer les ensembles de données imputées avec la fonction MIPCA en utilisant le nombre de dimensions précédemment calculé et la méthode bayésienne.  
  fonction **MIPCA (avec method.mi = "Bayes")**  
 
```{r pca}
res.bayesMIPCA = MIPCA(dHB,ncp=nb.kfold$ncp,verbose=FALSE,method.mi = "Bayes")
t1.acp=Sys.time() 
tdiff.acp= difftime(t1.acp, t0.acp, units="secs")
```

**Régression linéaire sur le jeu de données complété par MIPCA**

```{r lm_pca}
imppca= prelim(res.mi = res.bayesMIPCA, X = dHB)
fit= with(data=imppca,exp = lm(MEDV ~ CRIM + ZN + CHAS + NOX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT+I(LSTAT^2)))
res.pool=pool(fit)
summary(res.pool)

```


```{r pca2}
regpca=lm(MEDV ~ CRIM + ZN + CHAS + NOX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT+ I(LSTAT^2),data=res.pool)
summary(regpca)$adj.r.squared
```

Les valeurs des estimateurs sont très proches de celles trouvées par les autres méthodes d'imputation multiple. 

De même que pour les autres, le R² ajusté **`r regpca.adjr2`** est très proche du modèle avec des données manquantes. Les valeurs estimées et les résidus restent aussi très proches.

Nous avons testé 4 méthodes d'imputation, qui nous ont permis de faire des régressions avec le jeu de données complet. Ces régressions ont des résultats très proches en terme de R² ajusté, d'estimateurs et de résidus. Nous allons faire des diagnostics pour choisir le meilleur modèle d'imputation. 

# 4. Diagnostics et conclusion

## Diagnostic 1 : vérifier que la distribution des données imputées est similaire à celle des données d'origine. 

```{r diagnostics1}

densityplot(impregsto, main="Régression Stochastique", layout = c(2, 3))
densityplot(impforest, main="Forêts aléatoires", layout = c(2, 3))
densityplot(imppmm, main="Predictive mean matching", layout = c(2, 3))
densityplot(imppca, main="ACP", layout = c(2, 3))
```

Pour la variable CRIM, aucune méthode ne semble parvenir à reproduire la même distribution. Par contre pour INDUS, AGE et LSTAT, toutes les méthodes semblent réussir à le faire. 

Pour CHAS et ZN, les forêts aléatoires sont les plus proches de la vraie distribution.

Les méthodes de régression stochastique et d'ACP imputent des valeurs plus basses que celles observées dans la distribution originale des variables CRIM, ZN et AGE. Par contre, les imputations par les méthodes pmm et les forêts aléatoires respectent mieux la distribution originales des variables.

Pour la variable CRIM, les valeurs observées plus grandes se trouvent uniquement sur l'imputation par forêt aléatoire. 

Pour la variable AGE, la régression stochastique et ACP imputent des valeurs plus grandes (proches de 200) que celles observées (autour de 100).

## Diagnostic 2 : vérifier la convergence des algorithmes. 

La vérification de la convergence se fait à partir des graphiques de variation de la moyenne et de l'écart type, pour chaque  méthode, pour chaque itération et chaque donnée imputée.Pour que la convergence soit vérifiée, il faut que les différentes courbes se mélangent, sans une tendance particulière. C'est bien le cas pour nos graphiques; donc, nous n'avons pas de problème de convergence. (**cf.annexe 4.1**)

## Diagnostic 3 : vérifier l'ajustement du modèle d'imputation
Pour cela, nous traçons le graphe d'overimputation. Chaque donnée observée est supprimée et pour chacune d'entre elles, 100 valeurs sont predites (en utilisant la même méthode d'imputation choisie); la moyenne et des intervalles de confiance de 90% sont calculés pour ces valeurs prédites. 

Sur ces graphiques, la 1ère bissectrice (y=x) représente l'imputation parfaite. La qualité de l'imputation se mesure en observant la proximité des intervalles de confiance avec cette droite. On espère que 90% des intervalles traversent la 1ere bissectrice. La couleur des intervalles represente la fraction de données manquantes (entre 0 - 20% pour notre cas).  
Remarque : Comme CHAS n'est pas une variable continue, elle prend uniquement les valeurs 0 et 1, elle n'apparaît pas dans les graphiques.

**Ajustement de la regression stochastique :**

```{r diagnostics3a,warning=FALSE}
res.over1<-overimpute(impregsto,nnodes=7,plotvars = c(1,2,3,4,7,13),plotinds=sample(x = seq(nrow(dHB)),size = 100))

```

**Ajustement des forêts aléatoires :**

```{r diagnostics3b,warning=FALSE}
res.over2<-overimpute(impforest,nnodes=7,plotinds=sample(x = seq(nrow(dHB)),size = 100))

```

**Ajustement de la predictive mean matching :**

```{r diagnostics3c,warning=FALSE}

res.over3<-overimpute(imppmm,nnodes=7,plotinds=sample(x = seq(nrow(dHB)),size = 100))
```

**Ajustement de l'ACP :**

```{r diagnostics3d, warning=FALSE}

res.over4<-overimpute(imppca,nnodes=7,plotinds=sample(x = seq(nrow(dHB)),size = 100))
```

On observe que, quelque soit la méthode d'imputation, la plupart des intervalles de confiance des variables INDUS, AGE et LSTAT coupent la 1ere bissectrice  des méthodes : ce variables sont donc correctement imputées sur leurs données manquantes. 
Par contre, l'imputation des données maquantes sur CRIM et ZN est moins satisfaisante : particulièrement sur les methodes de regression stochastique, de forêts aléatoires et PMM.

### Conclusion

D'après les trois diagnostics réalisés, les 4 méthodes d'imputation présentées:

* Respectent bien la distribution des variables, même si la régression stochastique et l'ACP ne le font pas si bien que les autres.
* Ne présentent pas de problème de convergence
* Ne permettent pas de compléter de façon adéquate les variables CRIM et ZN (cf. overimputation).

Par contre, si on compare les méthodes d'imputation en terme de rapidité machine, la méthode APC est celle qui demande plus de temps.

Les coefficients R² ajustés (0.78) et les distributions des résidus (cf.**annexe 4.2**) de la regression linéaire sur le jeu de données complété sont très proches quelques soient les méthodes d'imputation utilisées . 

**Résumé des performances des méthodes d'imputation**  

| Critères      |Reg.stochastique |Random forest    | PMM           | ACP            |
|:------------- |:---------------:|:---------------:|:-------------:|:--------------:|
|D1:distribution|2/6              |5/6              |5/6            |2/6             |
|D2:convergence |OK               |OK               |OK             |OK              |
|D3:ajustement  |2/5              |4/5              |3/5            |5/5             |
|Temps machine (sec) |`r tdiff.regsto` |`r tdiff.rf`|`r tdiff.pmm`  |`r tdiff.acp`   |
|Modelisation de MEDV|              |              |               |                |
|               |R² ajusté = 0.78 |R² ajusté = 0.78 |R² ajusté = 0.78|              |
|               |résidus proches 0|résidus proches 0|résidus proches 0|              |

D'après ces éléments de performance, les imputations par forêts aléatoires ou PMM donnent les meilleurs résultats.  
  
  
Sur le dataset étudié, nous obtenons des résultats proches pour la régression linéaire de MEDV, avec ou sans imputation des données manquantes. Cela peut s'expliquer par :

  - la petite dimension du jeu de données.  
  - le modèle de regression linéaire choisi qui n'inclut pas 2 des variables avec données manquantes (INDUS et AGE).   
  
  
  
  
  
  
**_______________________________________________________________________________________________________________________________**    
  
  
# Annexes

### Annexe 1.1 : structure du jeu de données d'origine
```{r strucdHB}
summary(dHB)
```

### Annexe 1.2 : graphiques exploratoires du jeu de données
Les répresentations graphiques donnent, pour chaque variable explicative, la distribution sous forme d'histogramme et le nuage de points de la variable vs la valeur médiane du logement (en rouge la valeur moyenne = 22.5K$).

```{r angraDM, fig.height = 3.5}

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(CRIM,main=NULL,xlab="taux de criminalité",ylab="fréquence", col="blue")
plot(CRIM,MEDV,xlab="taux de criminalité",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et taux de criminalité")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(ZN,main=NULL,xlab="prop.terrains résidentiels",ylab="fréquence", col="blue")
plot(ZN,MEDV,xlab="prop.surface activité industrielle",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title(main="Valeur médiane du logement et proportion de terrains résidentiels")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(INDUS,main=NULL,xlab="prop.surface activité industrielle",ylab="fréquence", col="blue")
plot(INDUS,MEDV,xlab="prop.surface activité industrielle",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title(main="Valeur médiane du logement et proportion de surface d'activité industrielle")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(NOX,main=NULL,xlab="concentration oxyde d'azote (1/10M)",ylab="fréquence", col="blue")
plot(NOX,MEDV,xlab="concentration oxyde d'azote (1/10M)",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et concentration en oxyde d'azote")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(RM,main=NULL,xlab="nombre de chambre",ylab="fréquence", col="blue")
plot(RM,MEDV,xlab="nombre de chambre",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et nombre moyen de chambre")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(AGE,main=NULL,xlab="prop. construites avant 1940",ylab="fréquence", col="blue")
plot(AGE,MEDV,xlab="prop. construites avant 1940",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et proportion de propriétes construites avant 1940")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(DIS,main=NULL,xlab="distance moyenne aux centres d'emplois",ylab="fréquence", col="blue")
plot(DIS,MEDV,xlab="distance moyenne aux centres d'emplois",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et distance moyenne aux 5 centres d'emplois")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(TAX,main=NULL,xlab="taux d'imposition foncier (1/10K$",ylab="fréquence", col="blue")
plot(TAX,MEDV,xlab="taux d'imposition foncier (1/10K$",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et taux d'imposition foncier")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(PTRATIO,main=NULL,xlab="ratio élèves-enseignants",ylab="fréquence", col="blue")
plot(PTRATIO,MEDV,xlab="ratio élèves-enseignants",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et ratio élèves-enseignants")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(B,main=NULL,xlab="proportion de population afro-américaine",ylab="fréquence", col="blue")
plot(B,MEDV,xlab="proportion de population afro-américaine",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur du logement et proportion de population afro-américaine")

par(mfrow = c(1, 2), pch = 16, col="blue")
hist(LSTAT,main=NULL,xlab="proportion de population précaire",ylab="fréquence", col="blue")
plot(LSTAT,MEDV,xlab="proportion de population précaire",ylab="valeur médiane du logement (K$)")
abline(a=mean(MEDV),b=0, col="red")
par(mfrow = c(1, 1))
title("Valeur médiane du logement et proportion de population précaire")

par(mfrow = c(1, 2))
RADf=factor(RAD)
boxplot(MEDV~RADf,xlab="indice d'accessibilité aux autoroutes",ylab="valeur médiane du logement (K$)", col="blue")
abline(h=mean(MEDV), col="red")
CHASf=factor(CHAS)
boxplot(MEDV~CHASf,xlab="proximité avec la rivière Charles",ylab="valeur médiane du logement (K$)", col="blue")
abline(h=mean(MEDV), col="red")
```

### Annexe 1.3 : corrélation entre les variables du jeu de données
Les corrélations les plus significatives apparaissent en rouge.
```{r cor}
cordBH=round(cor(dHB,use="complete.obs"),2)
kable(ifelse(abs(cordBH)> 0.5, cell_spec(cordBH, "html", color = "red", 
    bold = T), cell_spec(cordBH, "html", color = "black")),format="markdown",digits=2)
 
```

### Annexe 1.4 : Jeu de données incomplet - regression lineaire MEDV sur l'ensemble des variables
```{r modDM}
reg1=lm(MEDV~.,data=dHB)
summary(reg1)

```

### Annexe 1.5 : Jeu de données incomplet -regression linéaire MEDV sur variables choisies par stepAIC
```{r stepwise}
dHB2 <-na.omit(dHB)
model <- lm(MEDV~.,data=dHB2) %>% stepAIC(trace = FALSE)
summary(model)
```

### Annexe 1.6 : Graphique des résidus du modèle
```{r res, fig.height = 3, fig.width = 5}
plot(model, 1)
```

### Annexe 1.7 : pairs plot
```{r pairs}
pairs(~ CRIM + ZN + CHAS + NOX + RM + DIS + MEDV, data = dHB)
pairs(~ RAD + TAX + PTRATIO + B + LSTAT + MEDV, data = dHB)
```

### Annexe 1.8 : Jeu de données incomplet - regression linéaire apres sélection de variables et incluant relation ordre 2 avec LSTAT
```{r multiple_regression, fig.height = 3, fig.width = 5}
dHB2 <-na.omit(dHB)
model2 <- lm(formula = MEDV ~ CRIM + ZN + CHAS + NOX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT+ I(LSTAT^2), data = dHB2)
summary(model2)
plot(model2,1)

```

### Annexe 2.1 : Matrixplot données manquantes

```{r inventDM3, fig.height = 3, fig.width = 5}
matrixplot(dHB,sortby="MEDV")
```


### Annexe 3.1 : Imputation - méthodologie
L'imputation correspond à l'action de convertir un échantillon incomplet en un échantillon complet. Le but de l'imputation multiple est d'affecter plusieurs fois des données manquantes, d'analyser les données complétées et ensuite d'intégrer les résultats des analyses.

Les 7 étapes de l'imputation:

**Etape 1** - Décider si supposition de MAR est plausible.  
(vu en partie 2)

**Etape 2** - Identifier la forme du modèle d'imputation.  
Le choix sera orienté par l’échelle de la variable à imputer, et intègre de préférence la connaissance de la relation entre les variables. L'algorithm MICE a besoin d'avoir une méthode univariée d'imputation pour chaque variable incomplète. 

**Etape 3** - Sélectionner le groupe de variables à inclure comme predicteurs dans le modèle d'imputation (fonction **mice**) 

```{r pred_matrix}
imp = mice(dHB, print = FALSE)
kable((imp$predictorMatrix),format="markdown")

```

Selon la matrice de résultat, CRIM sera prédit à partir de toutes les autres variables (indicateur = 1); idem pour ZN, INDUS, CHAS, AGE et LSTAT. Nous allons utiliser toutes les variables comme predicteurs. Cela est possible car le dataset est encore de taille raisonnable, (difficile sur les grands datasets , à cause de la multicolinearité ou de la capacité des machines) 

**Etape 4** - Imputer ou non des variables qui sont des fonctions d'autres variables incomplètes.     
Dans le cas de notre dataset, les variables avec des données manquantes ne sont pas des fonctions d'autres variables du dataset. Chacune répresente une thématique différente, utile pour l'estimation de la valeur de la maison.

**Etape 5** - Définir l'ordre d'imputation des variables (influe sur la convergence de l'algorithme). Par défaut, algorithme MICE impute les données incomplètes du dataset de gauche à droite. L'ordre est à changer si on a des soucis de convergence des algorithmes. 

**Etape 6** - Définir les imputations de départ et le nombre d'itérations 

**Etape 7** - Imputer et ajuster le modèle
L'imputation du dataset demande de faire des "essais-erreur", pour adapter et améliorer le modèle. Pour démarrer, il est conseillé de mettre m = 5 et l'augmenter lors de la dernière étape si on est déjà satisfait avec le modèle.

### Annexe 3.2 : Structure des données imputées par régression stochastique
```{r imputed1}
summary(dHBcompl.regsto)
summary(dHB)
```

### Annexe 3.3 : Structure des données imputées par forêts aléatoires
```{r imputed2}
summary(dHBcompl.rf)
```


### Annexe 3.4 : Structure des données imputées par pmm
```{r imputed3}
summary(dHBcompl.pmm)
```

### Annexe 3.5 : Imputation par ACP   
graphique des dimensions 
```{r dim_pca2}
plot(nb.kfold$criterion~names(nb.kfold$criterion),xlab="nb de critères",ylab="", type="b")
```

## Annexe 4.1 Diagnostic 2 : convergence des algorithmes. 

```{r diagnostics2}
plot(impregsto, main="Régression Stochastique", layout = c(2, 3))
plot(impforest, main="Forêts aléatoires", layout = c(2, 3))
plot(imppmm, main="Predictive mean matching", layout = c(2, 3))
#plot(imppca, main="ACP", layout = c(2, 3))
```


## Annexe 4.2 : Comparaison des résidus 

```{r residualspmm, fig.height = 8, fig.width = 8}
par(mfrow = c(3, 2))
plot(model, 1, "original")
plot(regrf,1,"Forêts aléatoires")
plot(regregsto,1,"Régression stochastique")
plot(regpmm, 1, "pmm")
plot(regpca, 1, "PCA")
```
