---
title: "Projet : Méthodes de traitement de données manquantes"
author: "Olga Silva / Marlène Chevalier"
date: "30/11/2019"  
output:
  html_document: default
---

<style type="text/css">
body{ /* Normal  */
font-size: 12px;
}
td {  /* Table  */
font-size: 12px;
}
h1.title {
font-size: 26px;
}

h1 { /* Header 1 */
font-size: 20px;
}
h2 { /* Header 2 */
font-size: 16px;
}
h3 { /* Header 3 */
font-size: 14px;
}
</style>



```{r setup, include=FALSE}

#packages utilisés
library(knitr)
library("missMDA")
library("VIM")
library(MASS)

library(fitdistrplus)
library("mice")
library(miceadds)
library("lavaan")
library(kableExtra)

# options globales
opts_chunk$set(echo=FALSE)

# chargement des données
dHB=read.table("HousingData.csv",sep=",",header=TRUE)
attach(dHB)
```

# Sujet : Valeur du logement en banlieue de Boston 

Il s'agit de traiter les données manquantes du fichier Boston Housing. Ce fichier décrit la situation des logements dans les villes de la banlieue de Boston. Il est constitué de 506 enregistrements et de 14 variables quantitatives (soit 7084 données) :

 - **CRIM** : Taux de criminalité par habitant.   
 - **ZN** : Proportion de terrains résidentiels pour des lots de plus de 25000 pieds carré (environ 2300m²).   
 - **INDUS** : Proportion d'espace, en acres, consacré aux affaires non commerciales (1 acre environ 4000m²).   
 - **CHAS** : Proximité avec la rivière Charles (=1 si en bord de rivière / =0 si éloigné de la rivière) 
  - **NOX** : Concentration en oxyde d'azote (1 pour 10 millions)  
  - **RM** : Nombre moyen de chambres par logement  
  - **AGE** : Proportion des propriétés construites avant 1940
  - **DIS** : Moyenne des distances aux 5 centres d'emploi de Boston  
  - **RAD** : Indice d'accessibilité aux autoroutes (de 1 à 8 et 24)  
  - **TAX** : Taux d'imposition foncier (1 pour 10 000$)  
  - **PTRATIO** : Ratio d'élèves-enseignants  
  - **B** : Proportion de population afro-américaine
  - **LSTAT** : Proportion de population précaire 
  - **MDEV** : Valeur médiane des habitations privées (en K$) 
  
Nous utiliserons ces données pour tenter d'expliquer la valeur médiane des habitations privées (MDEV) en fonction des autres variables du fichier.
  
# Exploration des données incomplètes
## Graphiques sur les données incomplètes

```{r graDM}
op=par(mfrow = c(2, 2), pch = 16, col="green")
{plot(CRIM,MEDV)
plot(ZN,MEDV)}
plot(INDUS,MEDV)
plot(NOX,MEDV)
par(op)
title("valeur des habitations (K$)")
par(mfrow = c(2, 2), pch = 16, col="green")
plot(RM,MEDV)
plot(AGE,MEDV)
plot(DIS,MEDV)
plot(RAD,MEDV)
par(mfrow = c(2, 2), pch = 16, col="green")
plot(TAX,MEDV)
plot(PTRATIO,MEDV)
plot(B,MEDV)
plot(LSTAT,MEDV)
par(mfrow = c(1, 2))
boxplot(CHAS,MEDV, col="green")
```

## Correlation des variables

```{r cor}
cordBH=cor(dHB,use="complete.obs")
kable(head(cordBH [1:14,1:14]),format="markdown",digits=2)


```

## Modélisation sur les données incomplètes

Nous commençons par examiner les résultats d'une regression linéaire de MEDV sur les autres variables .  Le modèle est correctement ajusté (R2=0.75) mais il semble que les variables INDUS et AGE ne soient pas pertinentes pour ce modèle. 

Nous voudrions maintenant utiliser la méthode "stepwise" pour choisir le meilleur modèle. Cependant, cette méthode ne fonctionne pas s'il y a des données manquantes. 
Afin de pouvoir l'utiliser, nous allons appliquer une méthode "listwise deletion" avec le risque de perte d'information et d'introduction de biais au dataset (cf. annexe 1.4). Ce modèle écarte les variables INDUS et AGE, comme l'original, avec un R2 ajusté très proche de l'original. 

# Inventaire des données manquantes 

## Structure des données manquantes

Nous allons utiliser la fonction 'md.pattern' du package mice pour analyser les données manquantes. Le résultat est une matrice, dans laquelle chaque ligne correspond à des structures de données manquantes et chaque colonne à une variable du fichier. Les lignes et les colonnes sont triées selon le niveau de complétude des données.   
A chaque ligne de la matrice (qui définit une structure de données manquantes du jeux de données) :  
  - la première colonne indique le nombre d'observations correspondant à la structure de données manquantes décrite ;   
  - la derniere colonne donne le nombre de variables incomplètes.

```{r inventDM1}
respattern=md.pattern(dHB,rotate.names = TRUE)
```

Au total, 120 observations sont manquantes : 20 pour chacune des variables CRIM, ZN, INDUS, CHAS, AGE, LSTAT. 

  - structure 1 : 394 observations pour lesquelles aucune donnée n'est manquante,
  - structure 2 : 18 observations pour lesquelles seule la donnée de la variable LSTAT est absente,
  - structure 3 : 18 observations pour lesquelles seule la donnée de la variable AGE est absente,   
  ...
  - structure 14 (dernière) : 1 observation pour laquelle les données des variables CRIM et CHAS sont absentes.
 
 
## Proportion de données manquantes

```{r inventDM2}
aggr(dHB,numbers=TRUE,bars=FALSE)
```



```{r inventDM3}
## Nuage de points 
#n=506
#txDM=1/6
#p=20/n
#indice=sample(1:n, size=round(n*txDM))
#indices=CRIM[is.na(CRIM)]
#CRIM[is.na(CRIM)]
#clr= rep("black",n)
#clr[indices]= "red"
#CRIM[indices]

#plot(CRIM,MEDV,col=clr, type="p")
#plot(ZN,MEDV)
#plot(INDUS,MEDV)
#plot(CHAS,MEDV)
#plot(MEDV)
#plot(LSTAT,MEDV)
```


## Lien variables manquantes / variables observées 
```{r manq,echo=FALSE}
fluxdBH=flux(dHB)[,2:3]
kable(head(fluxdBH),format="pandoc",digits=3)
```

L'influx d'une variable quantifie à quel point les données manquantes se connectent aux données observées des autres variables. 
L'outflux d'une variable quantifie à quel point les données observées se connectent aux données manquantes d'autres variables. 
D'une façon générale, des valeurs plus grandes pour les deux valeurs sont preferées. 


## Catégories de données manquantes

Rubin (1976) a classé les problèmes des données manquantes en trois catégories :  
  - Données manquantes de façon complètement aléatoire (missing completely at random, MCAR). La cause d'absence des données ne pas être liée au data. Même si pratique, cette hypothèse est peu réaliste.  
  - Données manquantes de façon aléatoire (missing at random, MAR). La probabilité d’absence de la valeur d’une variable dépend des valeurs prises par d’autres variables qui ont été observées  
  - Données manquantes de façon non aléatoire (missing not at random, MNAR). La probabilité d’absence de la valeur d’une variable dépend de variables qui n’ont pas été observées. 

La plupart des corrections simples fonctionnent sous l'hypothèse peu réaliste de MCAR. 
MAR est plus générale et plus réaliste que MCAR. La plupart des méthodes modernes de traitement des données manquantes partent de la supposition MAR.

Schafer and Graham (2002, 156) ont proposé que si le problème des données manquantes peut être traité en enlevant une petite partie de l'échantillon, alors la méthode peut être efficace.  


# Traitements des données manquantes

## Imputation simple

### Suppression par paire (pairwise deletion)

La suppression par paire est une autre alternative à la suppression simple des lignes avec des données manquantes. Avec cette méthode, il faut calculer la moyenne, variance et covariance de toutes les données observées.

Sous l’hypothèse de MCAR, les estimations de la moyenne, corrélations et covariances semblent consistants. Par contre, cette méthode pourrait poser des problèmes sur les variables très corrélées entre elles et si les données ne suivent pas une distribution normale. 

```{r modimpreg}
mu <- colMeans(dHB, na.rm = TRUE)
cv <- cov(dHB, use = "pairwise")

impreg <- lavaan("MEDV~1 + CRIM + ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT
              MEDV ~~ MEDV",
             sample.mean = mu, sample.cov = cv,
             sample.nobs = sum(complete.cases(dHB)))

```

D'autres solutions sont possibles aussi, comme remplacer les données manquantes par la moyenne de la variable, ou en faisant une régression avec les données observées. Même s’il s’agit des solutions rapides ces deux méthodes sont à éviter car cela modifie la corrélation entre les variables, la distribution, la variable est sous-estimée, entre autres problèmes.  

### Imputation par regression stochastique

Cette méthode consiste à imputer les données manquantes en utilisant la regression à laquelle on a ajouté du bruit. Cela permet de corriger le biais de corrélation qui existe par les méthodes plus rapides (decrites plus haut)

```{r modimpregsto}

impregsto <- mice(dHB, method = "norm.nob", m = 1, maxit = 1,
            seed = 1, print = FALSE)
summary(impregsto)
```


## Imputation multiple
L'imputation multiple est maintenant acceptée comme une méthode pour traiter des données non complètes. L'imputation correspond à l'action de convertir un échantillon non complet dans un échantillon complet. Le but de l'imputation multiple est d'imputer plusieurs fois les données manquantes, analyser les données complétés et ensuite intégrer les résultats des analyses. 



Nous avons 7 choix à faire pour specifier le modèle à utiliser pour l'imputation des données :

1 - Décider si supposition de MAR est plausible. D'habitude cette supposition est un bon départ. En plus il nous manque 22 % des données, nous sommes très loin du point problématique du 50%

2 - La forme du modèle d'imputation. Le choix sera orienté par l’échelle de la variable à imputer, et intègre de préférence la connaissance de la relation entre les variables
L'algorithm MICE a besoin d'avoir un méthode univarié d'imputation pour chaque variable incomplète. 

Pour notre dataset, nous avons 5 variables numériques avec des données manquantes : CRIM, ZN, INDUS, CHAS et AGE. 

La méthode par défaut est "pmm", Predictive mean matching, nous allons l'utiliser et la comparer avec rf, random forest.

3 - Le groupe de variables à inclure comme predicteurs dans le modèle d'imputation. 

```{r pred_matrix}
imp <- mice(dHB, print = FALSE)
imp$predictorMatrix

```

Selon la matrice de résultat, CRIM sera prédit à partir de toutes les autres variables; pareil pour ZN, INDUS, CHAS, AGE et LSTAT.
Cela est possible car le dataset est encore de taille raisonable, pour des grands datasets cela pourrait ne pas être possible, à cause de la multicolinearité ou de la capacité des machines. 

4 - Imputer ou non des variables qui sont des functions d'autres variables incomplètes. 
Dans le cas de notre dataset, les variables avec des données manquantes ne sont pas des fonctions d'autres variables du dataset. Chacune répresente une thématique différente, utile pour l'estimation de la valeur de la maison.

5 - L'ordre d'imputations des variables, car cela peut affecter  la convergence de l'algorithme. 
Par défaut, algorithme MICE impute les données incomplètes du dataset de gauche à droite. 

```{r convergence}
imp <- mice(dHB, seed = 62006, maxit = 20, print = FALSE)
plot(imp)

```

Voici les graphiques des variations de la moyenne et l'écart type, pour chaque itération et chaque donnée imputée. Pour la convergence, les différentes lignes doivent se mélanger librement sans une tendance définitive. C'est bien les cas pour nos graphiques, donc, nous n'avons pas des problèmes de convergence. 

6 - La mise en place des imputations de départ et le nombre d'itérations.

7 - la taille de m, le nombre d'ensembles des données imputées.  
Le choix de m : L'imputation du dataset demande de faire des "essais-erreur", pour adapter et améliorer le modèle. Pour démarrer il est conseillé de mettre m = 5 et l'augmenter lors de la dernière étape si on est déjà satisfait avec le modèle.

**attention la méthode est à modifier, j'ai laissé la méthode par défaut "pmm"!!!**

 

```{r mult_imputation}
#classic recommended workflow

imp <- mice(dHB, print = FALSE, m = 10, seed = 24415)
fit <- with(imp, lm(MEDV~1+CRIM + ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT))
#with : fill in the missing data and to analyze the data
est <- pool(fit)
est

```

La colonne "estimate" correspond à la valeur Q. Les colonnes "ubar", "b" et "t" sont les estimateurs des variances. "dfcom" correspond aux degrés de liberté du dataset comple, df est les degrés de liberté après la correction de Barnard-Rubin. Les trois dernières colonnes sont l'augmentation relative de la variance r, la proportion de la variance dû au non réponse λ et γ la fraction d'information manquante par paramètre. 

λ est différent de 1, c'est qui pouvait arriver si toute la variation était causé par les données manquantes. 

Les montants λ, r et γ, sont des indicateurs de la gravité du problème d'absence des données. Fractions d'information manquante de 0 - 0.2 sont interprétés comme "modeste", jusqu'à 0.3 comme "moyennement grand" et 0.5 comme "élevé" (Li, Raghunathan, and Rubin 1991). Des valeurs élevées indiquent un problème difficile dans lequel les inférences statistiques dépendent fortement de la manière dont les données manquantes ont été traitées.

Aucune de nos valeurs n'est supérieure à 0.09, le problème est alors modeste.

Des test de Wald et des intervalles de confiance pour chaque element de Q sont des résultats standard de chaque procédure statistique. 

```{r conf_interv}

summary(est, conf.int = TRUE)

```

Selon ces résultats, il semblerait que 'INDUS' et 'AGE' ne soient pas des prédicteurs statistiquement pertinents pour prédire MEDV, avec un erreur de type 1 de 5%
Ce résultat est cohérent par rapport aux résultats de la LM avec des données manquantes.


##Stepwise selection

L'imputation multiple consiste en trois phases :
1 - Imputation des données manquantes m fois
2 - Analyse de m datasets imputés
3 - Mise en commun des paramètres à travers m analyses

```{r model, results="hide"}

imp <- mice(dHB, seed = 123, m = 10, print = FALSE)
scope <- list(upper=~CRIM + ZN+INDUS+CHAS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT,
              lower = ~1)
expr <- expression(f1 <- lm(MEDV ~ 1),
                   f2 <- step(f1, scope = scope))
fit <- with(imp, expr)

formulas <- lapply(fit$analyses, formula)
terms <- lapply(formulas, terms)
votes <- unlist(lapply(terms, labels))
table(votes)

```
La function table compte le nombre de fois que chaque variable est présent dans les itérations. Le meilleure modèle semble être quand les 10 variables (B, CHAS, CRIM, DIS, LSTAT, NOX, PTRATIO, RAD, RM, TAX et ZN) sont presents. Ce modèle est cohérent avec l'exclusion d'INDUS et AGE pour le modèle. 

## critères d'évaluation

1 - Raw bias (RB) and percent bias (PB). Le biais brut de Q est defini comme la différence entre l'espérance de l'estimateur et le réél. Il doit être proche de zéro. 
2 - Coverage rate (CR). c'est la proportion des intervalles de confiance qui contiennent la valeur réelle. Il doit être proche de 0.95 
3 - Average width (AW), c'est un indicateur de l'efficacité statistique. La largeur doit être le plus petite possible, sans que le CR soit plus bas que la valeur nominale. 
Des méthodes sans biais et avec un bon CR sont connues comme "randomization-valid" (Rubin 1987b). 




# Conclusion


# Annexes  

### annexe 1.3 : regression lineaire MEDV sur l'ensemble des variables
```{r modDM}
reg1=lm(MEDV~.,data=dHB)
summary(reg1)

```

### annexe 1.4 : regression par stepwise MEDV sur l'ensemble des variables 
```{r stepwise}
dHB2 <-na.omit(dHB)
model <- lm(MEDV~.,data=dHB2) %>% stepAIC(trace = FALSE)
summary(model)

```
